{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cisco Corpus Search\n",
    "Given a query (question), search the best corresponding chunks in the whole Cisco Corpus using Facebook AI Similarity Search (FAISS) library. \n",
    "\n",
    "**Notebook instructions**\n",
    "\n",
    "    1. Make sure to create the corpus index before running this notebook. (NB: the index for the Cisco corpus has been created and saved in 'raid/antoloui/Master-thesis/_data/search/cisco/').\n",
    "    2. Run the Initialization cell. It will load in memory the FAISS index and the text chunks. (NB: You don't have to run this cell each time you makes a search, just once after starting the notebook).\n",
    "    3. Run the Search cell once you have typed a query to search for.\n",
    "\n",
    "**FAISS**: FAISS has excellent GPU implementation of \"brute-force\" kNN (meaning that no approximation techniques compromising the accuracy of the search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829b6346fa014a19925a74250ea6078a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='GPUs:', index=8, options=('0', '1', '2', '3', '4', '5', '6…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#                                  CODE\n",
    "#---------------------------------------------------------------------------\n",
    "def format_time(elapsed):\n",
    "    \"\"\"\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    \"\"\"\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "\n",
    "def init(index_dir, n_gpu):\n",
    "    \"\"\"\n",
    "    Load FAISS index and all text chunks.\n",
    "    \"\"\"\n",
    "    # Load FAISS index.\n",
    "    if os.path.exists(os.path.join(index_dir, \"cisco_corpus.index\")):\n",
    "        index = faiss.read_index(os.path.join(index_dir, \"cisco_corpus.index\"))\n",
    "        if n_gpu > 0 and faiss.get_num_gpus() > 0:\n",
    "            if n_gpu > faiss.get_num_gpus(): n_gpu = faiss.get_num_gpus()\n",
    "            co = faiss.GpuMultipleClonerOptions()  # If using multiple GPUs, enable sharding so that the dataset is divided across the GPUs rather than replicated.\n",
    "            co.shard = True\n",
    "            index = faiss.index_cpu_to_all_gpus(index, co=co, ngpu=n_gpu)  # Convert CPU index to GPU index.\n",
    "    else:\n",
    "        print(\"Error: no index found in {}... Make sure to create the index before searching in corpus. Exiting...\".format(index_dir))\n",
    "        sys.exit(0)\n",
    "        \n",
    "    # Load text chunks.\n",
    "    if os.path.exists(os.path.join(index_dir, \"cisco_chunks.txt\")):\n",
    "        with open(os.path.join(index_dir, \"cisco_chunks.txt\"), \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Error: no chunks found in {}... Make sure to create the index before searching in corpus. Exiting...\".format(index_dir))\n",
    "        sys.exit(0)\n",
    "    \n",
    "    return index, chunks\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#                                 WIDGETS\n",
    "#---------------------------------------------------------------------------\n",
    "# GPUs dropdown list.\n",
    "nb_gpus = list(range(faiss.get_num_gpus()+1))\n",
    "nb_gpus = list(map(str, nb_gpus))\n",
    "choose_gpus = widgets.Dropdown(\n",
    "    options=nb_gpus,\n",
    "    value='8',\n",
    "    description='GPUs:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Button for loading FAISS index.\n",
    "init_btn = widgets.Button(description='Init')\n",
    "init_out = widgets.Output()\n",
    "def init_btn_eventhandler(obj):\n",
    "    with init_out:\n",
    "        print(\"\\nLoading FAISS index...\")\n",
    "        t0 = time.time()\n",
    "        index, chunks = init(index_dir='/raid/antoloui/Master-thesis/_data/search/cisco/', n_gpu=int(choose_gpus.value))\n",
    "        print(\"  Done.  -  Took: {}\".format(format_time(time.time() - t0)))\n",
    "init_btn.on_click(init_btn_eventhandler)\n",
    "\n",
    "# Display widgets.\n",
    "box = widgets.HBox([choose_gpus, init_btn])\n",
    "widgets.VBox([box, init_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91269a50e30e47518b40cb3fdf3cbbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', description='Query:', placeholder='Type your query'), IntSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#                                  CODE\n",
    "#---------------------------------------------------------------------------\n",
    "def encode_queries(sentences, \n",
    "                   model_name_or_path='/raid/antoloui/Master-thesis/_models/netbert-final/'):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, get the embeddings of their embeddings with NetBERT\n",
    "    as the average of the word embeddings of the last layer (padding tokens excluded).\n",
    "    \"\"\"\n",
    "    # Loading pretrained model/tokenizer.\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "    model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True) # Will output all hidden_states.\n",
    "\n",
    "    #Tokenizing sentences.\n",
    "    tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "    lengths = [len(i) for i in tokenized]\n",
    "    max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "    #Padding/Truncating sentences to max_len tokens.\n",
    "    padded = pad_sequences(tokenized, maxlen=max_len, dtype=\"long\", \n",
    "                           value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    #Creating attention masks.\n",
    "    attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "    # Converting inputs to torch tensors.\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    # Encoding sentences.\n",
    "    with torch.no_grad():\n",
    "        # output is a 2-tuple where:\n",
    "        #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "        #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "        #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "    last_hidden_states = output[0]\n",
    "    sentence_embeddings = [torch.mean(embeddings[:torch.squeeze((masks == 1).nonzero(), dim=1).shape[0]], dim=0).numpy() for embeddings, masks in zip(last_hidden_states, attention_mask)]\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    \n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def search_corpus(query, index, chunks, topk):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Encode query with NetBERT.\n",
    "    query_embedding = encode_queries([query])[0]\n",
    "    \n",
    "    # Search topk results.\n",
    "    result_dist, result_idx = index.search(query_embedding.reshape(1,768), k=topk)\n",
    "    \n",
    "    # Display topk results.\n",
    "    for i, (idx, dist) in enumerate(zip(result_idx[0], result_dist[0])):\n",
    "        print(\"\\nTop {} result - (L2: {:.2f})\".format(i+1, dist))\n",
    "        print(\"--------------------------\")\n",
    "        print(chunks[idx])\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#                                 WIDGETS\n",
    "#---------------------------------------------------------------------------\n",
    "# Text widget for typing in query.\n",
    "choose_query = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your query',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Slider widget for choosing topk results value.\n",
    "choose_topk = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Topk:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "# Button widget for launching the search.\n",
    "search_btn = widgets.Button(description='Search')\n",
    "search_out = widgets.Output()\n",
    "def search_btn_eventhandler(obj):\n",
    "    with search_out:\n",
    "        search_corpus(query=choose_query.value, index=index, chunks=chunks, topk=choose_topk.value)\n",
    "search_btn.on_click(search_btn_eventhandler)\n",
    "\n",
    "# Display widgets.\n",
    "box = widgets.HBox([choose_query, choose_topk, search_btn])\n",
    "widgets.VBox([box, search_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
