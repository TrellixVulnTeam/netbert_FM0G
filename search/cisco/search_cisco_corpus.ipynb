{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cisco Corpus Search\n",
    "Given a query (question), search the best corresponding chunks in the whole Cisco Corpus using Facebook AI Similarity Search (FAISS) library. \n",
    "\n",
    "**FAISS**: FAISS has excellent GPU implementation of \"brute-force\" kNN (meaning that no approximation techniques compromising the accuracy of the search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "\n",
    "The 'Initialization' cell will load in memory the FAISS index created for the whole Cisco corpus. Hence, that cell needs to be run only once after starting the notebook.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Run the cell.\n",
    "2. Choose the type of FAISS index.\n",
    "3. Choose the number of GPUs you want to use for the search (the more the faster).\n",
    "4. Click on the 'Init' button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82d97c7a3624a018d409988d8756b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Index:', options=('L2', 'Inner-Product', 'Cosine'), value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import faiss\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#                                  CODE\n",
    "#---------------------------------------------------------------------------\n",
    "def format_time(elapsed):\n",
    "    \"\"\"\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    \"\"\"\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "\n",
    "def init(dirpath, method, n_gpu):\n",
    "    \"\"\"\n",
    "    Load FAISS index and all text chunks.\n",
    "    \"\"\"\n",
    "    # Get the method.\n",
    "    index_name = 'l2' if method=='L2' else 'ip' if method=='Inner-Product' else 'cos'\n",
    "    \n",
    "    # Load FAISS index.\n",
    "    if os.path.exists(os.path.join(dirpath, index_name + \".index\")):\n",
    "        index = faiss.read_index(os.path.join(dirpath, index_name + \".index\"))\n",
    "        if n_gpu > 0 and faiss.get_num_gpus() > 0:\n",
    "            if n_gpu > faiss.get_num_gpus(): n_gpu = faiss.get_num_gpus()\n",
    "            co = faiss.GpuMultipleClonerOptions()  # If using multiple GPUs, enable sharding so that the dataset is divided across the GPUs rather than replicated.\n",
    "            co.shard = True\n",
    "            index = faiss.index_cpu_to_all_gpus(index, co=co, ngpu=n_gpu)  # Convert CPU index to GPU index.\n",
    "    else:\n",
    "        print(\"Error: no index found in {}... Make sure to create the index before searching in corpus. Exiting...\".format(dirpath))\n",
    "        sys.exit(0)\n",
    "        \n",
    "    # Load text chunks.\n",
    "    if os.path.exists(os.path.join(dirpath, \"chunks.txt\")):\n",
    "        with open(os.path.join(dirpath, \"chunks.txt\"), \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Error: no chunks found in {}... Make sure to create the index before searching in corpus. Exiting...\".format(dirpath))\n",
    "        sys.exit(0)\n",
    "    \n",
    "    return index, chunks\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#                                 WIDGETS\n",
    "#---------------------------------------------------------------------------\n",
    "# Index dropdown list.\n",
    "options = ['L2', 'Inner-Product', 'Cosine']\n",
    "choose_index = widgets.Dropdown(\n",
    "    options=options,\n",
    "    value='L2',\n",
    "    description='Index:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# GPUs dropdown list.\n",
    "nb_gpus = list(range(faiss.get_num_gpus()+1))\n",
    "nb_gpus = list(map(str, nb_gpus))\n",
    "choose_gpus = widgets.Dropdown(\n",
    "    options=nb_gpus,\n",
    "    value='8',\n",
    "    description='GPUs:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Button for loading FAISS index.\n",
    "init_btn = widgets.Button(description='Init')\n",
    "init_out = widgets.Output()\n",
    "def init_btn_eventhandler(obj):\n",
    "    init_out.clear_output()  # Clear output.\n",
    "    gpus = int(choose_gpus.value) # Get the GPUs value from dropdown.\n",
    "    with init_out:\n",
    "        print(\"\\nLoading FAISS index with {} GPUs...\".format(gpus))\n",
    "        t0 = time.time()\n",
    "        global index, chunks\n",
    "        index, chunks = init(dirpath='/raid/antoloui/Master-thesis/_data/search/cisco/',\n",
    "                             method=choose_index.value, \n",
    "                             n_gpu=gpus)\n",
    "        print(\"  Done.  -  Took: {}\".format(format_time(time.time() - t0)))\n",
    "init_btn.on_click(init_btn_eventhandler)\n",
    "\n",
    "# Display widgets.\n",
    "box = widgets.HBox([choose_index, choose_gpus, init_btn])\n",
    "widgets.VBox([box, init_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search\n",
    "\n",
    "Given a query, the 'Search' cell will return the top-k most similar text chunks from the Cisco corpus.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Run the cell.\n",
    "2. Type a query.\n",
    "3. Choose the number of results to display.\n",
    "4. Click on the 'Search' button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455df668d67d4ddea4e0920398e2d6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', description='Query:', layout=Layout(width='50%'), placeholder='Ty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#                                  CODE\n",
    "#---------------------------------------------------------------------------\n",
    "def encode_queries(model, tokenizer, sentences):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, get the embeddings of their embeddings with NetBERT\n",
    "    as the average of the word embeddings of the last layer (padding tokens excluded).\n",
    "    \"\"\"\n",
    "    #Tokenizing sentences.\n",
    "    tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "    lengths = [len(i) for i in tokenized]\n",
    "    max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "    #Padding/Truncating sentences to max_len tokens.\n",
    "    padded = pad_sequences(tokenized, maxlen=max_len, dtype=\"long\", \n",
    "                           value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    #Creating attention masks.\n",
    "    attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "    # Converting inputs to torch tensors.\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    # Encoding sentences.\n",
    "    with torch.no_grad():\n",
    "        # output is a 2-tuple where:\n",
    "        #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "        #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "        #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "    last_hidden_states = output[0]\n",
    "    sentence_embeddings = [torch.mean(embeddings[:torch.squeeze((masks == 1).nonzero(), dim=1).shape[0]], dim=0).numpy() for embeddings, masks in zip(last_hidden_states, attention_mask)]\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    \n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def load_model(model_name_or_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Loading pretrained model/tokenizer.\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "    model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True) # Will output all hidden_states.\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def search_corpus(model, tokenizer, query, index, chunks, topk):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    start_total = time.time()\n",
    "    \n",
    "    # Encode query with NetBERT.\n",
    "    start_encode = time.time()\n",
    "    query_embedding = encode_queries(model, tokenizer, [query])[0]\n",
    "    end_encode = time.time()\n",
    "    \n",
    "    # Search topk results.\n",
    "    start_search = time.time()\n",
    "    result_dist, result_idx = index.search(query_embedding.reshape(1,768), k=topk)\n",
    "    end_search = time.time()\n",
    "    \n",
    "    end_total = time.time()\n",
    "    # Display search time.\n",
    "    print(\"\\n** Search time **\")\n",
    "    print(\"  Encoding query: {:.3f}\".format(end_encode-start_encode))\n",
    "    print(\"  Faiss search: {:.3f}\".format(end_search-start_search))\n",
    "    print(\"  (Total search time: {:.3f})\".format(end_total - start_total))\n",
    "    \n",
    "    # Display topk results.\n",
    "    for i, (idx, dist) in enumerate(zip(result_idx[0], result_dist[0])):\n",
    "        print(\"\\nTop {} result - (Distance: {:.3f})\".format(i+1, dist))\n",
    "        print(\"---------------------------\")\n",
    "        print(chunks[idx])\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#                                 WIDGETS\n",
    "#---------------------------------------------------------------------------\n",
    "# Text widget for typing in query.\n",
    "choose_query = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your query',\n",
    "    description='Query:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "\n",
    "# Slider widget for choosing topk results value.\n",
    "choose_topk = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Topk:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "# Button widget for launching the search.\n",
    "search_btn = widgets.Button(description='Search')\n",
    "search_out = widgets.Output()\n",
    "model, tokenizer = load_model(model_name_or_path='/raid/antoloui/Master-thesis/_models/netbert-final/')\n",
    "def search_btn_eventhandler(obj):\n",
    "    search_out.clear_output()  # Clear output.\n",
    "    global model\n",
    "    global tokenizer\n",
    "    with search_out:\n",
    "        search_corpus(model=model, \n",
    "                      tokenizer=tokenizer,\n",
    "                      query=choose_query.value, \n",
    "                      index=index, \n",
    "                      chunks=chunks, \n",
    "                      topk=choose_topk.value)\n",
    "search_btn.on_click(search_btn_eventhandler)\n",
    "\n",
    "# Display widgets.\n",
    "box = widgets.HBox([choose_query, choose_topk, search_btn])\n",
    "widgets.VBox([box, search_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which protocols are examples of TCP/IP transport layer protocols?\n",
    "# Which statement describes part of the process of how a LAN switch decides to forward a frame destined for a broadcast MAC address?\n",
    "# Which value in the configuration register controls how the router boots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of '.ipynb_checkpoints': 0.0000 GB\n",
      "Size of 'chunks.txt': 14.7516 GB\n",
      "Size of 'cos.index': 25.6577 GB\n",
      "Size of 'ip.index': 25.6577 GB\n",
      "Size of 'l2.index': 25.6577 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = '/raid/antoloui/Master-thesis/_data/search/cisco/'\n",
    "files = os.listdir(directory).sort()\n",
    "\n",
    "\n",
    "# Check size of an index.\n",
    "total_size = 0\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    file_path = directory + filename\n",
    "    size = os.path.getsize(file_path)/(1024*1024*1024)\n",
    "    total_size += size\n",
    "    print(\"Size of '{}': {:.4f} GB\".format(filename, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8968001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
