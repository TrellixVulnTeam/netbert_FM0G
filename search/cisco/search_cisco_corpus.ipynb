{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Cisco Corpus\n",
    "Given a query (question), search the best corresponding chunks in the whole Cisco Corpus using Facebook AI Similarity Search (FAISS) library. FAISS has excellent GPU implementation of \"brute-force\" kNN (meaning that no approximation techniques compromising the accuracy of the search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def search_corpus(query, indir, n_gpu):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Load FAISS index.\n",
    "    if os.path.exists(os.path.join(indir, \"cisco_corpus.index\")):\n",
    "        index = faiss.read_index(os.path.join(indir, \"cisco_corpus.index\"))\n",
    "        if n_gpu > 0:\n",
    "            co = faiss.GpuMultipleClonerOptions()  # If using multiple GPUs, enable sharding so that the dataset is divided across the GPUs rather than replicated.\n",
    "            co.shard = True\n",
    "            index = faiss.index_cpu_to_all_gpus(index, co=co, ngpu=n_gpu)  # Convert CPU index to GPU index.\n",
    "    else:\n",
    "        print(\"Error: no index found in {}... Make sure to create the index before searching in corpus. Exiting...\".format(indir))\n",
    "        sys.exit(0)\n",
    "        \n",
    "    # Load the chunks.\n",
    "    if os.path.exists(os.path.join(indir, \"chunks.txt\")):\n",
    "        with open(os.path.join(indir, \"chunks.txt\"), \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Error: no chunks found in {}... Make sure to create the index before searching in corpus. Exiting...\".format(indir))\n",
    "        sys.exit(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create FAISS GPU index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def load_embeddings(input_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Create dataframe.\n",
    "    cols = ['feat'+str(i+1) for i in range(768)]\n",
    "    cols.append('Chunk')\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    #Concat embeddings from all files.\n",
    "    filepaths = glob.glob(input_dir + '*.h5')\n",
    "    for file in tqdm(filepaths, desc='Files'):\n",
    "        df_file = pd.read_hdf(file)\n",
    "        df_file['Chunk'] = df_file['Chunk'].astype(str)\n",
    "        df = pd.concat([df, df_file], ignore_index=True, sort=False)\n",
    "\n",
    "    #Check for duplicated chunks in the concatenated dataframe.\n",
    "    df.drop_duplicates(subset=['Chunk'], keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Get chunks and their embeddings.\n",
    "    chunks = df.iloc[:,-1].values\n",
    "    embeddings = df.iloc[:,:-1].values\n",
    "    embeddings = np.ascontiguousarray(embeddings, dtype=np.float32) # Necessary for FAISS indexing afterwards.\n",
    "    \n",
    "    return chunks, embeddings\n",
    "\n",
    "\n",
    "def create_faiss_index(vecs, method='l2', n_gpu=0):\n",
    "    \"\"\"\n",
    "    Create FAISS index on GPU(s).\n",
    "    To create a GPU index with FAISS, one first needs to create it on CPU then copy it on GPU. \n",
    "    Note that a \"flat\" index means that it is brute-force, with no approximation techniques.\n",
    "    \"\"\"\n",
    "    # Build flat CPU index given the chosen method.\n",
    "    if method=='l2':\n",
    "        index = faiss.IndexFlatL2(vecs.shape[1])  # Exact Search for L2\n",
    "    elif method=='ip':\n",
    "        index = faiss.IndexFlatIP(vecs.shape[1])  # Exact Search for Inner Product (also for cosine, just normalize vectors beforehand)\n",
    "    else:\n",
    "        print(\"Error: Please choose between L2 distance ('l2') or Inner Product ('ip') as brute-force method for exact search. Exiting...\")\n",
    "        sys.exit(0)\n",
    "    \n",
    "    # Convert to flat GPU index.\n",
    "    if n_gpu > 0:\n",
    "        co = faiss.GpuMultipleClonerOptions()  # If using multiple GPUs, enable sharding so that the dataset is divided across the GPUs rather than replicated.\n",
    "        co.shard = True\n",
    "        index = faiss.index_cpu_to_all_gpus(index, co=co, ngpu=n_gpu)  # Convert CPU index to GPU index.\n",
    "    \n",
    "    # Add vectors to GPU index.\n",
    "    index.add(vecs)\n",
    "    \n",
    "    # Convert back to cpu index (needed for saving it to disk).\n",
    "    index = faiss.index_gpu_to_cpu(index)\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def create_corpus_index(input_dir, output_dir, n_gpu):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"\\nLoad all embeddings of Cisco corpus from {}...\".format(input_dir))\n",
    "    chunks, embeddings = load_embeddings(input_dir)\n",
    "    \n",
    "    print(\"Create FAISS (GPU) index...\")\n",
    "    index = create_faiss_index(vecs=embeddings, n_gpu=n_gpu)\n",
    "    \n",
    "    print(\"\\nSave index to {}...\".format(output_dir))\n",
    "    faiss.write_index(index, os.path.join(output_dir, \"cisco_corpus.index\"))\n",
    "    \n",
    "    print(\"\\nSave chunks to {}...\".format(output_dir))\n",
    "    with open(os.path.join(output_dir,\"cisco_chunks.txt\"), \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    print(\"\\nFAISS index created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load all embeddings of Cisco corpus from /raid/antoloui/Master-thesis/_data/embeddings/test/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481b3c74cb6a480699781fa7b28088b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='  Files', max=2.0, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create FAISS (GPU) index...\n",
      "\n",
      "Save index to /raid/antoloui/Master-thesis/_data/embeddings/test/...\n",
      "\n",
      "Save chunks to /raid/antoloui/Master-thesis/_data/embeddings/test/...\n",
      "\n",
      "FAISS index created.\n"
     ]
    }
   ],
   "source": [
    "create_corpus_index(input_dir='/raid/antoloui/Master-thesis/_data/embeddings/test/', \n",
    "                    output_dir='/raid/antoloui/Master-thesis/_data/embeddings/test/',\n",
    "                    n_gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
