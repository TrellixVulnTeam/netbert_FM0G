{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data extraction\n",
    "\n",
    "Extract text contained in json files and save it in a dataframe for further pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638c62c6c7a64188b14f3f714f8fec92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 - Keys: {'text', 'uri'} - Documents: 11699\n",
      "File 2 - Keys: {'text', 'uri'} - Documents: 32072\n",
      "File 3 - Keys: {'text', 'uri'} - Documents: 8225\n",
      "File 4 - Keys: {'text', 'uri'} - Documents: 77258\n",
      "File 5 - Keys: {'text', 'uri'} - Documents: 46079\n",
      "File 6 - Keys: {'text', 'uri'} - Documents: 28106\n",
      "File 7 - Keys: {'text', 'uri'} - Documents: 27391\n",
      "File 8 - Keys: {'text', 'uri'} - Documents: 24143\n",
      "File 9 - Keys: {'text', 'uri'} - Documents: 22223\n",
      "File 10 - Keys: {'text', 'uri'} - Documents: 20979\n",
      "File 11 - Keys: {'text', 'uri'} - Documents: 57160\n",
      "File 12 - Keys: {'text', 'uri'} - Documents: 85900\n",
      "File 13 - Keys: {'text', 'uri'} - Documents: 793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the different keys in the json file\n",
    "for id_file in tqdm(range(1,14)):\n",
    "    file_path = \"./../Data/Original_data/\" + str(id_file) + \".json\"\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Loop over each document\n",
    "        keys = []\n",
    "        for i, doc in enumerate(data):\n",
    "            for key, value in doc.items():\n",
    "                keys.append(key)\n",
    "\n",
    "        myset = set(keys)\n",
    "        print(\"File {} - Keys: {} - Documents: {}\".format(id_file, myset, len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eafd220ac7f4e619c94a7156d2bd3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file 1...\n",
      "Extracting text from 11699 documents in file...\n",
      "Loading file 2...\n",
      "Extracting text from 32072 documents in file...\n",
      "Loading file 3...\n",
      "Extracting text from 8225 documents in file...\n",
      "Loading file 4...\n",
      "Extracting text from 77258 documents in file...\n",
      "Loading file 5...\n",
      "Extracting text from 46079 documents in file...\n",
      "Loading file 6...\n",
      "Extracting text from 28106 documents in file...\n",
      "Loading file 7...\n",
      "Extracting text from 27391 documents in file...\n",
      "Loading file 8...\n",
      "Extracting text from 24143 documents in file...\n",
      "Loading file 9...\n",
      "Extracting text from 22223 documents in file...\n",
      "Loading file 10...\n",
      "Extracting text from 20979 documents in file...\n",
      "Loading file 11...\n",
      "Extracting text from 57160 documents in file...\n",
      "Loading file 12...\n",
      "Extracting text from 85900 documents in file...\n",
      "Loading file 13...\n",
      "Extracting text from 793 documents in file...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Americas Headquarters: Cisco Systems, Inc., 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the latest version of the Cisco Small Busi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WebEx Meeting Center User Guide  For Hosts, Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78-4019959-01 Rev D  Prisma II 1550 nm SuperQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Release Notes for Cisco RV130/RV130W Firmware ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442014</th>\n",
       "      <td>Cisco Unified Communications System for IP Tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442015</th>\n",
       "      <td>Cisco Unified Communications System for IP Tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442016</th>\n",
       "      <td>Cisco Unified Communications System for IP Tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442017</th>\n",
       "      <td>Cisco Unified Communications System for IP Tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442018</th>\n",
       "      <td>Cisco Unified Communications System for IP Tel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442019 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text\n",
       "0       Americas Headquarters: Cisco Systems, Inc., 17...\n",
       "1       For the latest version of the Cisco Small Busi...\n",
       "2       WebEx Meeting Center User Guide  For Hosts, Pr...\n",
       "3       78-4019959-01 Rev D  Prisma II 1550 nm SuperQA...\n",
       "4       Release Notes for Cisco RV130/RV130W Firmware ...\n",
       "...                                                   ...\n",
       "442014  Cisco Unified Communications System for IP Tel...\n",
       "442015  Cisco Unified Communications System for IP Tel...\n",
       "442016  Cisco Unified Communications System for IP Tel...\n",
       "442017  Cisco Unified Communications System for IP Tel...\n",
       "442018  Cisco Unified Communications System for IP Tel...\n",
       "\n",
       "[442019 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for id_file in tqdm(range(1, 14)):\n",
    "    file_path = \"./../Data/Original_data/\" + str(id_file) + \".json\"\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        print(\"Loading file {}...\".format(id_file))\n",
    "        data = json.load(f)  # data is a list of dict of the form: {'text':['...'], 'uri':['...']}\n",
    "\n",
    "        print(\"Extracting text from {} documents in file...\".format(len(data), id_file))\n",
    "        for i, doc in enumerate(data):\n",
    "            text = doc.get('text') # Get the text of the current doc\n",
    "            if text is not None:\n",
    "                row_dict = {'Text': text[0]}\n",
    "                rows.append(row_dict)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Data Processing\n",
    "\n",
    "In order to use \"create_pretraining_data.py\" from BERT repository, the input must be a plain text file, with one sentence per line (it is important that these be actual sentences for the \"next sentence prediction\" task). They advise to perform sentence segmentation with an off-the-shelf NLP toolkit such as spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all special characters in the texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Text = df.Text.replace('\\s+', ' ', regex=True)  # Remove duplicate spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence segmentation with spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(contents)\n",
    "sentences = list(doc.sents)\n",
    "for sentence in sentences:\n",
    "    print(str(sentence)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences segmentation with nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "sentences = sent_tokenize(contents)\n",
    "for sentence in sentences:\n",
    "    print(sentence+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
