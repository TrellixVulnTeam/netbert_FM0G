{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data extraction\n",
    "\n",
    "Extract text contained in json files and save it in a dataframe for further pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "import spacy, en_core_web_sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb7fc9fc28f433f856d113b3fddd927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='./../Data/Original_data/1.json' mode='r' encoding='utf-8'>\n",
      "File 1 - Keys: {'text', 'uri'} - Documents: 11699\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/2.json' mode='r' encoding='utf-8'>\n",
      "File 2 - Keys: {'text', 'uri'} - Documents: 32072\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/3.json' mode='r' encoding='utf-8'>\n",
      "File 3 - Keys: {'text', 'uri'} - Documents: 8225\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/4.json' mode='r' encoding='utf-8'>\n",
      "File 4 - Keys: {'text', 'uri'} - Documents: 77258\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/5.json' mode='r' encoding='utf-8'>\n",
      "File 5 - Keys: {'text', 'uri'} - Documents: 46079\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/6.json' mode='r' encoding='utf-8'>\n",
      "File 6 - Keys: {'text', 'uri'} - Documents: 28106\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/7.json' mode='r' encoding='utf-8'>\n",
      "File 7 - Keys: {'text', 'uri'} - Documents: 27391\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/8.json' mode='r' encoding='utf-8'>\n",
      "File 8 - Keys: {'text', 'uri'} - Documents: 24143\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/9.json' mode='r' encoding='utf-8'>\n",
      "File 9 - Keys: {'text', 'uri'} - Documents: 22223\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/10.json' mode='r' encoding='utf-8'>\n",
      "File 10 - Keys: {'text', 'uri'} - Documents: 20979\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/11.json' mode='r' encoding='utf-8'>\n",
      "File 11 - Keys: {'text', 'uri'} - Documents: 57160\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/12.json' mode='r' encoding='utf-8'>\n",
      "File 12 - Keys: {'text', 'uri'} - Documents: 85900\n",
      "<_io.TextIOWrapper name='./../Data/Original_data/13.json' mode='r' encoding='utf-8'>\n",
      "File 13 - Keys: {'text', 'uri'} - Documents: 793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the different keys in the json file\n",
    "for id_file in tqdm(range(1,14)):\n",
    "    file_path = \"./../Data/Original_data/\" + str(id_file) + \".json\"\n",
    "\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Loop over each document\n",
    "        keys = []\n",
    "        for i, doc in enumerate(data):\n",
    "            for key, value in doc.items():\n",
    "                keys.append(key)\n",
    "\n",
    "        myset = set(keys)\n",
    "        print(\"File {} - Keys: {} - Documents: {}\".format(id_file, myset, len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f8ad37b16440a18c1e5bf8073c8b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file 1...\n",
      "Extracting text from 11699 documents in file...\n",
      "Loading file 2...\n",
      "Extracting text from 32072 documents in file...\n",
      "Loading file 3...\n",
      "Extracting text from 8225 documents in file...\n",
      "Loading file 4...\n",
      "Extracting text from 77258 documents in file...\n",
      "Loading file 5...\n",
      "Extracting text from 46079 documents in file...\n",
      "Loading file 6...\n",
      "Extracting text from 28106 documents in file...\n",
      "Loading file 7...\n",
      "Extracting text from 27391 documents in file...\n",
      "Loading file 8...\n",
      "Extracting text from 24143 documents in file...\n",
      "Loading file 9...\n",
      "Extracting text from 22223 documents in file...\n",
      "Loading file 10...\n",
      "Extracting text from 20979 documents in file...\n",
      "Loading file 11...\n",
      "Extracting text from 57160 documents in file...\n",
      "Loading file 12...\n",
      "Extracting text from 85900 documents in file...\n",
      "Loading file 13...\n",
      "Extracting text from 793 documents in file...\n",
      "\n",
      "Creating dataframe...\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for id_file in tqdm(range(1, 14)):\n",
    "    file_path = \"./../Data/Original_data/\" + str(id_file) + \".json\"\n",
    "\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        print(\"Loading file {}...\".format(id_file))\n",
    "        data = json.load(f)  # data is a list of dict of the form: {'text':['...'], 'uri':['...']}\n",
    "\n",
    "        print(\"Extracting text from {} documents in file...\".format(len(data), id_file))\n",
    "        for i, doc in enumerate(data):\n",
    "            text = doc.get('text') # Get the text of the current doc\n",
    "            if text is not None:\n",
    "                row_dict = {'Text': text[0], 'Length': len(text[0])}\n",
    "                rows.append(row_dict)\n",
    "\n",
    "print(\"Creating dataframe...\")\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of characters in a doc: 2621440\n",
      "Total number of docs: 442019\n"
     ]
    }
   ],
   "source": [
    "print(\"Max number of characters in a doc: {}\".format(df.Length.max()))\n",
    "print(\"Total number of docs: {}\".format(len(df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "In order to use \"create_pretraining_data.py\" from BERT repository, the input must be a plain text file, with one sentence per line and one blank line between documents:\n",
    "\n",
    "  * One sentence per line. These should ideally be actual sentences, not entire paragraphs or arbitrary spans of text. (Because we use the sentence boundaries for the \"next sentence prediction\" task).\n",
    "  * Blank lines between documents. Document boundaries are needed so that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  \n",
    "They advise to perform sentence segmentation with an off-the-shelf NLP toolkit such as spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Corpus Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning corpus of text...\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning corpus of text...\")\n",
    "df.Text = df.Text.replace('\\s+', ' ', regex=True)  # Remove duplicate spaces\n",
    "df.Text = df.Text.str.encode('ascii', 'ignore').str.decode('utf-8')   # Encode in ascii to remove weird characters such as \\uf0a7\n",
    "df.Text = df.Text.str.lower()  # Lower case all strings (I have noticed a better segmentation by doing that)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_segmentation(doc_text):\n",
    "    \"\"\"\n",
    "    Given a string, segment it by sentences.\n",
    "    \"\"\"\n",
    "    nlp = en_core_web_sm.load()\n",
    "    nlp.max_length = 2621500  # because larger document has a size of 2621440 char\n",
    "    doc = nlp(doc_text)\n",
    "    sentences = list(doc.sents)\n",
    "    return [sent.text for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Sentence cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_cleaning(list_sent):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # If line begins with a number, remove the number   \n",
    "    list_sent = [sent.split(maxsplit=1)[1] if (sent.split(maxsplit=1)[0].isdigit() and len(sent.split(maxsplit=1)) > 1) else sent for sent in list_sent]\n",
    "    \n",
    "    # If line begins with a special char, remove that char\n",
    "    spec_char = set(',?;.:/=+%`¨^*$€-_())°!§\\'\\\"&@#~®†ºπ‡¬≈©◊~∞µ…÷≠<>')\n",
    "    list_sent = [sent.split(maxsplit=1)[1] if (len(sent.split(maxsplit=1)) > 1 and sent[0] in spec_char) else sent for sent in list_sent]\n",
    "    \n",
    "    # Keep only sentences that have less that 15 special characters\n",
    "    list_sent = [sent for sent in list_sent if max([sent.count(c) for c in spec_char]) < 15]\n",
    "\n",
    "    # Keep only sentences with more than 2 words\n",
    "    list_sent = [sent for sent in list_sent if len(sent.split()) > 2]\n",
    "    return list_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Plain text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_convert(list_sent):\n",
    "    \"\"\"\n",
    "    Given a list of string sentences, return one unique string where\n",
    "    sentences are separated by newlines.\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(list_sent)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Apply all cleaning functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e880233596fb4f1ba2836df3f088cabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=442019.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Segmenting sentences...\")\n",
    "df['Text'] = df['Text'].progress_apply(sent_segmentation)\n",
    "print(\"Cleaning sentences...\")\n",
    "df['Text'] = df['Text'].progress_apply(sent_cleaning)\n",
    "print(\"Concatenating all sentences...\")\n",
    "df['Text'] = df['Text'].progress_apply(sent_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Concatenate all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = \"\\n\\n\".join(df[\"Text\"])\n",
    "with open(\"../Data/output.txt\", \"w+\") as f:\n",
    "    f.write(final_text)\n",
    "print(\"DONE !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
