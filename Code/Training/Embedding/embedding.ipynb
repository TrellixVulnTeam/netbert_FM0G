{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased', output_hidden_states=True, cache_dir ='../cache') # Will output all hidden_states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "    â€¢ Changes in membership of any endpoint to the device pool or association of any endpoint to the redundancy group.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = './data/sample.txt'\n",
    "with open(filepath) as f:\n",
    "    sentences = f.readlines()\n",
    "    \n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "    [101, 794, 21395, 1107, 5467, 1104, 1251, 1322, 7587, 1106, 1103, 4442, 4528, 1137, 3852, 1104, 1251, 1322, 7587, 1106, 1103, 1894, 22902, 7232, 1372, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(tokenized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 123\n",
      "Example:\n",
      "    [  101   794 21395  1107  5467  1104  1251  1322  7587  1106  1103  4442\n",
      "  4528  1137  3852  1104  1251  1322  7587  1106  1103  1894 22902  7232\n",
      "  1372   119   102     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Define length of longest sentence in our dataset.\n",
    "max_len = 0\n",
    "for i in tokenized:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print(\"Maximum length: {}\".format(max_len))\n",
    "        \n",
    "# Pad each tokenized sentence according to the maximum length.\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized])\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(padded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(attention_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the input to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of hidden_states: torch.Size([13, 370, 123, 768])\n",
      "   - Number of layers (+1 with initial token embeddings): 13\n",
      "   - Number of sentences: 370\n",
      "   - Number of tokens in a sentence: 123\n",
      "   - Dimension of an embedding : 768\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # output is a 2-tuple where:\n",
    "    #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "    #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "    #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "    output = model(input_ids, attention_mask=attention_mask) \n",
    "\n",
    "# Get individual components of the output.\n",
    "last_hidden_states = output[0]\n",
    "pooler_output = output[1]\n",
    "hidden_states = output[2]\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Print dimensions of output.\n",
    "print(\"Dimensions of hidden_states: {}\".format(hidden_states.size()))\n",
    "print(\"   - Number of layers (+1 with initial token embeddings): {}\".format(hidden_states.size(0)))\n",
    "print(\"   - Number of sentences: {}\".format(hidden_states.size(1)))\n",
    "print(\"   - Number of tokens in a sentence: {}\".format(hidden_states.size(2)))\n",
    "print(\"   - Dimension of an embedding : {}\".format(hidden_states.size(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
