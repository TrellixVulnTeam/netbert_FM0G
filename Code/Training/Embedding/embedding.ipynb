{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased', output_hidden_states=True, cache_dir ='../cache') # Will output all hidden_states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "Packets consist of two kinds of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>An enterprise private network is a network tha...</td>\n",
       "      <td>Geographic scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Circuit-switched networks: In circuit switched...</td>\n",
       "      <td>Network performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>A wide area network (WAN) is a computer networ...</td>\n",
       "      <td>Geographic scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>In 1977, Xerox Network Systems (XNS) was devel...</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The most striking example of an overlay networ...</td>\n",
       "      <td>Network topology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1977, first long-distance fiber network deploy...</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>The following list gives examples of network p...</td>\n",
       "      <td>Network performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Any data sent across a network requires time t...</td>\n",
       "      <td>Network performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Wireless LAN, also widely known as WLAN or WiF...</td>\n",
       "      <td>Communication protocols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>There are many ways to measure the performance...</td>\n",
       "      <td>Network performance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "105  An enterprise private network is a network tha...   \n",
       "132  Circuit-switched networks: In circuit switched...   \n",
       "104  A wide area network (WAN) is a computer networ...   \n",
       "19   In 1977, Xerox Network Systems (XNS) was devel...   \n",
       "42   The most striking example of an overlay networ...   \n",
       "18   1977, first long-distance fiber network deploy...   \n",
       "131  The following list gives examples of network p...   \n",
       "129  Any data sent across a network requires time t...   \n",
       "84   Wireless LAN, also widely known as WLAN or WiF...   \n",
       "134  There are many ways to measure the performance...   \n",
       "\n",
       "                       Label  \n",
       "105         Geographic scale  \n",
       "132      Network performance  \n",
       "104         Geographic scale  \n",
       "19                   History  \n",
       "42          Network topology  \n",
       "18                   History  \n",
       "131      Network performance  \n",
       "129      Network performance  \n",
       "84   Communication protocols  \n",
       "134      Network performance  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = './data/computer_network.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "sentences = df.Sentence.values\n",
    "    \n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n\\n{}\".format(random.choice(sentences)))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "[CLS] Packets consist of two kinds of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between. [SEP]\n",
      "['[CLS]', 'Pack', '##ets', 'consist', 'of', 'two', 'kinds', 'of', 'data', ':', 'control', 'information', 'and', 'user', 'data', '(', 'payload', ')', '.', 'The', 'control', 'information', 'provides', 'data', 'the', 'network', 'needs', 'to', 'deliver', 'the', 'user', 'data', ',', 'for', 'example', ',', 'source', 'and', 'destination', 'network', 'addresses', ',', 'error', 'detection', 'codes', ',', 'and', 'se', '##quencing', 'information', '.', 'Typically', ',', 'control', 'information', 'is', 'found', 'in', 'packet', 'header', '##s', 'and', 'trailers', ',', 'with', 'payload', 'data', 'in', 'between', '.', '[SEP]']\n",
      "[101, 14667, 6248, 8296, 1104, 1160, 7553, 1104, 2233, 131, 1654, 1869, 1105, 4795, 2233, 113, 21586, 114, 119, 1109, 1654, 1869, 2790, 2233, 1103, 2443, 2993, 1106, 7852, 1103, 4795, 2233, 117, 1111, 1859, 117, 2674, 1105, 7680, 2443, 11869, 117, 7353, 11432, 9812, 117, 1105, 14516, 27276, 1869, 119, 16304, 117, 1654, 1869, 1110, 1276, 1107, 17745, 23103, 1116, 1105, 24760, 117, 1114, 21586, 2233, 1107, 1206, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization in multiple steps.\n",
    "marked_text = [\"[CLS] \" + sent + \" [SEP]\" for sent in sentences]\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in marked_text]\n",
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_text]\n",
    "\n",
    "# Tokenization in one step.\n",
    "tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "# Example.\n",
    "print(\"Example:\\n\")\n",
    "random.seed(42)\n",
    "print(random.choice(marked_text))\n",
    "random.seed(42)\n",
    "print(random.choice(tokenized_text))\n",
    "random.seed(42)\n",
    "print(random.choice(indexed_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 219\n",
      "Example:\n",
      "    [  101 14667  6248  8296  1104  1160  7553  1104  2233   131  1654  1869\n",
      "  1105  4795  2233   113 21586   114   119  1109  1654  1869  2790  2233\n",
      "  1103  2443  2993  1106  7852  1103  4795  2233   117  1111  1859   117\n",
      "  2674  1105  7680  2443 11869   117  7353 11432  9812   117  1105 14516\n",
      " 27276  1869   119 16304   117  1654  1869  1110  1276  1107 17745 23103\n",
      "  1116  1105 24760   117  1114 21586  2233  1107  1206   119   102     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Define length of longest sentence in our dataset.\n",
    "max_len = 0\n",
    "for i in tokenized:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print(\"Maximum length: {}\".format(max_len))\n",
    "        \n",
    "# Pad each tokenized sentence according to the maximum length.\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized])\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(padded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(attention_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the input to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of hidden_states: torch.Size([13, 155, 219, 768])\n",
      "   - Number of layers (+1 with initial token embeddings): 13\n",
      "   - Number of sentences: 155\n",
      "   - Number of tokens in a sentence: 219\n",
      "   - Dimension of an embedding : 768\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # output is a 2-tuple where:\n",
    "    #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "    #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "    #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get individual components of the output.\n",
    "last_hidden_states = output[0]\n",
    "pooler_output = output[1]\n",
    "hidden_states = output[2]\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Print dimensions of output.\n",
    "print(\"Dimensions of hidden_states: {}\".format(hidden_states.size()))\n",
    "print(\"   - Number of layers (+1 with initial token embeddings): {}\".format(hidden_states.size(0)))\n",
    "print(\"   - Number of sentences: {}\".format(hidden_states.size(1)))\n",
    "print(\"   - Number of tokens in a sentence: {}\".format(hidden_states.size(2)))\n",
    "print(\"   - Dimension of an embedding : {}\".format(hidden_states.size(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([155, 219, 13, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch around the “layers” and “tokens” dimensions with permute.\n",
    "hidden_states = hidden_states.permute(1,2,0,3)\n",
    "hidden_states.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average last hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 768)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "sentence_embeddings = [torch.mean(embeddings, dim=0).numpy() for embeddings in last_hidden_states]\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colors(x):\n",
    "    \"\"\"\n",
    "    Generate x tuples of RGB colors.\n",
    "    \"\"\"\n",
    "    return [tuple(np.random.randint(250, size=3)) for i in range(0,x)]\n",
    "\n",
    "\n",
    "# Load SummaryWriter (will output to ./runs/ directory by default).\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Get the labels and generate one color for each.\n",
    "labels = df.Label.unique().tolist()\n",
    "colors = generate_colors(len(labels))\n",
    "\n",
    "# Associate each sentence with the color of its label.\n",
    "label_img = torch.zeros(len(sentence_embeddings), 3, 32, 32)\n",
    "for i in range(len(sentence_embeddings)):\n",
    "    # Get color of that label.\n",
    "    sentence_label = df.loc[i,'Label']\n",
    "    idx = labels.index(sentence_label)\n",
    "    color = colors[idx]\n",
    "    \n",
    "    # Set color.\n",
    "    label_img[i,0,:,:] = float(color[0])\n",
    "    label_img[i,1,:,:] = float(color[1])\n",
    "    label_img[i,2,:,:] = float(color[2])\n",
    "\n",
    "# Write to tensorboard\n",
    "writer.add_embedding(sentence_embeddings, metadata=df.Sentence.values, label_img=label_img)\n",
    "\n",
    "# Close writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum the last four layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 123 x 768\n"
     ]
    }
   ],
   "source": [
    "# Take the first sentence as a sample.\n",
    "sample_sentence = hidden_states[0]  # `sample_sentence` is a [123 x 13 x 768] tensor.\n",
    "\n",
    "# Stores the token vectors, with shape [123 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...(`token` is a [13 x 768] tensor)\n",
    "for token in sample_sentence:\n",
    "    \n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 'hidden_states' is a torch.Size([24, 13, 768]) tensor.\n",
      " - 'token_vecs_sum' is a (24, 768) array.\n",
      "0 [CLS]\n",
      "1 After\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 r\n",
      "12 ##ob\n",
      "13 ##ber\n",
      "14 was\n",
      "15 seen\n",
      "16 fishing\n",
      "17 on\n",
      "18 the\n",
      "19 Mississippi\n",
      "20 river\n",
      "21 bank\n",
      "22 .\n",
      "23 [SEP]\n",
      "Vector similarity for  *similar*  meanings:  0.90\n",
      "Vector similarity for *different* meanings:  0.68\n"
     ]
    }
   ],
   "source": [
    "# Example sentence.\n",
    "sentence = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Tokenization in multiple steps.\n",
    "marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Tokenization in one step.\n",
    "tokenized = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "tokenized = torch.tensor([tokenized])\n",
    "\n",
    "# Encode it.\n",
    "with torch.no_grad():\n",
    "    output = model(tokenized)\n",
    "    \n",
    "# Get all hidden states.\n",
    "hidden_states = output[2]\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "hidden_states = torch.squeeze(hidden_states, dim=1)\n",
    "hidden_states = hidden_states.permute(1,0,2)\n",
    "print(\" - 'hidden_states' is a {} tensor.\".format(hidden_states.size()))\n",
    "\n",
    "# Sum the last four layers.\n",
    "token_vecs_sum = []\n",
    "for token in hidden_states:\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "print(\" - 'token_vecs_sum' is a ({}, {}) array.\".format(len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\n",
    "# Get idx of words of interest 'bank'.\n",
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)\n",
    "\n",
    "# Calculate the cosine similarity between the word bank in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
