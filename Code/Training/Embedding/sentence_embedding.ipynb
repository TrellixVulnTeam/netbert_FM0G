{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "model_name_or_path = '../models/netbert/checkpoint-1027000/'  #'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True, cache_dir ='../cache') # Will output all hidden_states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "Packets consist of two kinds of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Apart from any physical transmission media, ne...</td>\n",
       "      <td>Network nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Wireless bridges: Can be used to join LANs or ...</td>\n",
       "      <td>Network nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Most routing algorithms use only one network p...</td>\n",
       "      <td>Routing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>A ring network: each node is connected to its ...</td>\n",
       "      <td>Network topology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In 1963, J. C. R. Licklider sent a memorandum ...</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>A personal area network (PAN) is a computer ne...</td>\n",
       "      <td>Geographic scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Computer communication links that do not suppo...</td>\n",
       "      <td>Network packet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Terrestrial microwave – Terrestrial microwave ...</td>\n",
       "      <td>Network links</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Both cases have a large round-trip delay time,...</td>\n",
       "      <td>Network links</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>A network interface controller (NIC) is comput...</td>\n",
       "      <td>Network nodes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence             Label\n",
       "63   Apart from any physical transmission media, ne...     Network nodes\n",
       "73   Wireless bridges: Can be used to join LANs or ...     Network nodes\n",
       "123  Most routing algorithms use only one network p...           Routing\n",
       "35   A ring network: each node is connected to its ...  Network topology\n",
       "9    In 1963, J. C. R. Licklider sent a memorandum ...           History\n",
       "92   A personal area network (PAN) is a computer ne...  Geographic scale\n",
       "27   Computer communication links that do not suppo...    Network packet\n",
       "54   Terrestrial microwave – Terrestrial microwave ...     Network links\n",
       "62   Both cases have a large round-trip delay time,...     Network links\n",
       "64   A network interface controller (NIC) is comput...     Network nodes"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = './data/computer_network.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "sentences = df.Sentence.values\n",
    "    \n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n\\n{}\".format(random.choice(sentences)))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "[CLS] Packets consist of two kinds of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between. [SEP]\n",
      "['[CLS]', 'Pack', '##ets', 'consist', 'of', 'two', 'kinds', 'of', 'data', ':', 'control', 'information', 'and', 'user', 'data', '(', 'payload', ')', '.', 'The', 'control', 'information', 'provides', 'data', 'the', 'network', 'needs', 'to', 'deliver', 'the', 'user', 'data', ',', 'for', 'example', ',', 'source', 'and', 'destination', 'network', 'addresses', ',', 'error', 'detection', 'codes', ',', 'and', 'se', '##quencing', 'information', '.', 'Typically', ',', 'control', 'information', 'is', 'found', 'in', 'packet', 'header', '##s', 'and', 'trailers', ',', 'with', 'payload', 'data', 'in', 'between', '.', '[SEP]']\n",
      "[101, 14667, 6248, 8296, 1104, 1160, 7553, 1104, 2233, 131, 1654, 1869, 1105, 4795, 2233, 113, 21586, 114, 119, 1109, 1654, 1869, 2790, 2233, 1103, 2443, 2993, 1106, 7852, 1103, 4795, 2233, 117, 1111, 1859, 117, 2674, 1105, 7680, 2443, 11869, 117, 7353, 11432, 9812, 117, 1105, 14516, 27276, 1869, 119, 16304, 117, 1654, 1869, 1110, 1276, 1107, 17745, 23103, 1116, 1105, 24760, 117, 1114, 21586, 2233, 1107, 1206, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization in multiple steps.\n",
    "marked_text = [\"[CLS] \" + sent + \" [SEP]\" for sent in sentences]\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in marked_text]\n",
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_text]\n",
    "\n",
    "# Tokenization in one step.\n",
    "tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "# Example.\n",
    "print(\"Example:\\n\")\n",
    "random.seed(42)\n",
    "print(random.choice(marked_text))\n",
    "random.seed(42)\n",
    "print(random.choice(tokenized_text))\n",
    "random.seed(42)\n",
    "print(random.choice(indexed_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 219\n",
      "Example:\n",
      "    [  101 14667  6248  8296  1104  1160  7553  1104  2233   131  1654  1869\n",
      "  1105  4795  2233   113 21586   114   119  1109  1654  1869  2790  2233\n",
      "  1103  2443  2993  1106  7852  1103  4795  2233   117  1111  1859   117\n",
      "  2674  1105  7680  2443 11869   117  7353 11432  9812   117  1105 14516\n",
      " 27276  1869   119 16304   117  1654  1869  1110  1276  1107 17745 23103\n",
      "  1116  1105 24760   117  1114 21586  2233  1107  1206   119   102     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Define length of longest sentence in our dataset.\n",
    "max_len = 0\n",
    "for i in tokenized:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print(\"Maximum length: {}\".format(max_len))\n",
    "        \n",
    "# Pad each tokenized sentence according to the maximum length.\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized])\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(padded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(attention_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the input to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of hidden_states: torch.Size([13, 155, 219, 768])\n",
      "   - Number of layers (+1 with initial token embeddings): 13\n",
      "   - Number of sentences: 155\n",
      "   - Number of tokens in a sentence: 219\n",
      "   - Dimension of an embedding : 768\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # output is a 2-tuple where:\n",
    "    #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "    #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "    #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get individual components of the output.\n",
    "last_hidden_states = output[0]\n",
    "pooler_output = output[1]\n",
    "hidden_states = output[2]\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Print dimensions of output.\n",
    "print(\"Dimensions of hidden_states: {}\".format(hidden_states.size()))\n",
    "print(\"   - Number of layers (+1 with initial token embeddings): {}\".format(hidden_states.size(0)))\n",
    "print(\"   - Number of sentences: {}\".format(hidden_states.size(1)))\n",
    "print(\"   - Number of tokens in a sentence: {}\".format(hidden_states.size(2)))\n",
    "print(\"   - Dimension of an embedding : {}\".format(hidden_states.size(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([155, 219, 13, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch around the “layers” and “tokens” dimensions with permute.\n",
    "hidden_states = hidden_states.permute(1,2,0,3)\n",
    "hidden_states.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average last hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "sentence_embeddings = [torch.mean(embeddings, dim=0).numpy() for embeddings in last_hidden_states]\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colors(x):\n",
    "    \"\"\"\n",
    "    Generate x random colors.\n",
    "    \"\"\"\n",
    "    return [tuple(np.random.uniform(low=0.0, high=1.0, size=3)) for i in range(x)]\n",
    "\n",
    "\n",
    "# Load SummaryWriter (will output to ./runs/ directory by default).\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Get the labels and generate one color for each.\n",
    "labels = df.Label.unique().tolist()\n",
    "\n",
    "# Associate each sentence with the color of its label.\n",
    "label_img = torch.zeros(len(sentence_embeddings), 3, 32, 32)\n",
    "colors = generate_colors(len(labels))\n",
    "for i in range(len(sentence_embeddings)):\n",
    "    # Get color of that label.\n",
    "    sentence_label = df.loc[i,'Label']\n",
    "    idx = labels.index(sentence_label)\n",
    "    color = colors[idx]\n",
    "    \n",
    "    # Set color.\n",
    "    label_img[i,0,:,:] = color[0]\n",
    "    label_img[i,1,:,:] = color[1]\n",
    "    label_img[i,2,:,:] = color[2]\n",
    "\n",
    "# Write to tensorboard\n",
    "writer.add_embedding(sentence_embeddings, metadata=df.Sentence.values, label_img=label_img)\n",
    "\n",
    "# Close writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum the last four layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 123 x 768\n"
     ]
    }
   ],
   "source": [
    "# Take the first sentence as a sample.\n",
    "sample_sentence = hidden_states[0]  # `sample_sentence` is a [123 x 13 x 768] tensor.\n",
    "\n",
    "# Stores the token vectors, with shape [123 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...(`token` is a [13 x 768] tensor)\n",
    "for token in sample_sentence:\n",
    "    \n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 'hidden_states' is a torch.Size([24, 13, 768]) tensor.\n",
      " - 'token_vecs_sum' is a (24, 768) array.\n",
      "0 [CLS]\n",
      "1 After\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 r\n",
      "12 ##ob\n",
      "13 ##ber\n",
      "14 was\n",
      "15 seen\n",
      "16 fishing\n",
      "17 on\n",
      "18 the\n",
      "19 Mississippi\n",
      "20 river\n",
      "21 bank\n",
      "22 .\n",
      "23 [SEP]\n",
      "Vector similarity for  *similar*  meanings:  0.90\n",
      "Vector similarity for *different* meanings:  0.68\n"
     ]
    }
   ],
   "source": [
    "# Example sentence.\n",
    "sentence = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Tokenization in multiple steps.\n",
    "marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Tokenization in one step.\n",
    "tokenized = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "tokenized = torch.tensor([tokenized])\n",
    "\n",
    "# Encode it.\n",
    "with torch.no_grad():\n",
    "    output = model(tokenized)\n",
    "    \n",
    "# Get all hidden states.\n",
    "hidden_states = output[2]\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "hidden_states = torch.squeeze(hidden_states, dim=1)\n",
    "hidden_states = hidden_states.permute(1,0,2)\n",
    "print(\" - 'hidden_states' is a {} tensor.\".format(hidden_states.size()))\n",
    "\n",
    "# Sum the last four layers.\n",
    "token_vecs_sum = []\n",
    "for token in hidden_states:\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "print(\" - 'token_vecs_sum' is a ({}, {}) array.\".format(len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\n",
    "# Get idx of words of interest 'bank'.\n",
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)\n",
    "\n",
    "# Calculate the cosine similarity between the word bank in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
