{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS\n",
    "Facebook AI Similarity Search (FAISS) library, which has excellent GPU implementation of \"brute-force\" kNN (meaning that no approximation techniques compromising the accuracy of the search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FAISS index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load CCNA chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create FAISS index\n",
    "\n",
    "To create a GPU index with FAISS, one first needs to create it on CPU then copy it on GPU. Note that a \"flat\" index means that it is brute-force, with no approximation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpu_index(vecs, n_gpus, method='l2'):\n",
    "    \"\"\"\n",
    "    Create FAISS index on GPU(s).\n",
    "    \"\"\"\n",
    "    print(\"  Number of available GPUs: {}  -  Using: {}\".format(faiss.get_num_gpus(), n_gpus))\n",
    "    \n",
    "    print(\"  Building flat CPU index...\")\n",
    "    if method=='l2':\n",
    "        cpu_index = faiss.IndexFlatL2(vecs.shape[1])  # Exact Search for L2\n",
    "    elif method=='ip':\n",
    "        cpu_index = faiss.IndexFlatIP(vecs.shape[1])  # Exact Search for Inner Product (also for cosine, just normalize vectors beforehand)\n",
    "    else:\n",
    "        print(\"Error: Please choose between L2 distance ('l2') or Inner Product ('ip') as brute-force method for exact search. Exiting...\")\n",
    "        sys.exit(0)\n",
    "    \n",
    "    print(\"  Converting to flat GPU index...\")\n",
    "    co = faiss.GpuMultipleClonerOptions()  # If using multiple GPUs, enable sharding so that the dataset is divided across the GPUs rather than replicated.\n",
    "    co.shard = True\n",
    "    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index,co=co, ngpu=n_gpus)  # Convert CPU index to GPU index.\n",
    "    \n",
    "    print(\"  Adding vectors to GPU index...\")\n",
    "    gpu_index.add(vecs)\n",
    "\n",
    "    return gpu_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT-base FAISS index...\n",
      "  Number of available GPUs: 8  -  Using: 1\n",
      "  Building flat CPU index...\n",
      "  Converting to flat GPU index...\n",
      "  Adding vectors to GPU index...\n",
      "Building NetBERT FAISS index...\n",
      "  Number of available GPUs: 8  -  Using: 1\n",
      "  Building flat CPU index...\n",
      "  Converting to flat GPU index...\n",
      "  Adding vectors to GPU index...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding BERT-base FAISS index...\")\n",
    "netbert_index = create_gpu_index(vecs=bert_embeddings,\n",
    "                             n_gpus=1,\n",
    "                             method='l2')\n",
    "\n",
    "print(\"\\nBuilding NetBERT FAISS index...\")\n",
    "netbert_index = create_gpu_index(vecs=netbert_embeddings,\n",
    "                             n_gpus=1,\n",
    "                             method='l2')\n",
    "print(\"\\nDONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FAISS search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load and encode questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions(filepath):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, sep=\";\")\n",
    "    questions = df.Question.values\n",
    "    return df, questions\n",
    "\n",
    "\n",
    "def encode_sentences(model_name_or_path, cache, sentences):\n",
    "    \"\"\"\n",
    "    Given a list of sentences and a model, get the embeddings of theses sentences\n",
    "    as the average of the word embeddings of the last layer.\n",
    "    \"\"\"\n",
    "    print(\"   Loading pretrained model/tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "    model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True, cache_dir=cache) # Will output all hidden_states.\n",
    "\n",
    "    print(\"   Tokenizing sentences...\")\n",
    "    tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "    lengths = [len(i) for i in tokenized]\n",
    "    max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "    print(\"   Padding/Truncating sentences to {} tokens...\".format(max_len))\n",
    "    padded = pad_sequences(tokenized, maxlen=max_len, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    print(\"   Creating attention masks...\")\n",
    "    attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "    print(\"   Converting inputs to torch tensors...\")\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    print(\"   Encoding sentences...\")\n",
    "    with torch.no_grad():\n",
    "        # output is a 2-tuple where:\n",
    "        #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "        #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "        #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "    last_hidden_states = output[0]\n",
    "    sentence_embeddings = [torch.mean(embeddings[:torch.squeeze((masks == 1).nonzero(), dim=1).shape[0]], dim=0).numpy() for embeddings, masks in zip(last_hidden_states, attention_mask)]\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading questions...\n",
      "\n",
      "Encoding questions with BERT-base...\n",
      "   Loading pretrained model/tokenizer...\n",
      "   Tokenizing sentences...\n",
      "   Padding/Truncating sentences to 125 tokens...\n",
      "   Creating attention masks...\n",
      "   Converting inputs to torch tensors...\n",
      "   Encoding sentences...\n",
      "\n",
      "Encoding questions with NetBERT...\n",
      "   Loading pretrained model/tokenizer...\n",
      "   Tokenizing sentences...\n",
      "   Padding/Truncating sentences to 125 tokens...\n",
      "   Creating attention masks...\n",
      "   Converting inputs to torch tensors...\n",
      "   Encoding sentences...\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading questions...\")\n",
    "df, questions = load_questions('/raid/antoloui/Master-thesis/Data/QA/questions.csv')\n",
    "\n",
    "print(\"\\nEncoding questions with BERT-base...\")\n",
    "bert_embeddings = encode_sentences(model_name_or_path='bert-base-cased', \n",
    "                                   cache='/raid/antoloui/Master-thesis/Code/_cache/',\n",
    "                                   sentences=questions)\n",
    "\n",
    "print(\"\\nEncoding questions with NetBERT...\")\n",
    "netbert_embeddings = encode_sentences(model_name_or_path='/raid/antoloui/Master-thesis/Code/_models/netbert-830000/', \n",
    "                                      cache='/raid/antoloui/Master-thesis/Code/_cache/',\n",
    "                                      sentences=questions)\n",
    "print(\"\\nDONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Search with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the question i\n",
    "i = 4\n",
    "question = questions.iloc[4].question\n",
    "\n",
    "# Find the 5 most similar chunks.\n",
    "D, I = gpu_index.search(vecs[i].reshape(1,768), k=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
