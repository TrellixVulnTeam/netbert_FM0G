{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS\n",
    "Facebook AI Similarity Search (FAISS) library, which has excellent GPU implementation of \"brute-force\" kNN (meaning that no approximation techniques compromising the accuracy of the search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import faiss\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def load_chunks(filepath):\n",
    "    \"\"\"\n",
    "    Load the chunk embeddings of the text file.\n",
    "    \"\"\"\n",
    "    # Load dataframe.\n",
    "    df = pd.read_hdf(filepath)\n",
    "    \n",
    "    # Get chunks and their embeddings.\n",
    "    chunks = df.iloc[:,-1].values\n",
    "    embeddings = df.iloc[:,:-1].values\n",
    "    embeddings = np.ascontiguousarray(embeddings, dtype=np.float32) # Necessary for FAISS indexing afterwards.\n",
    "    \n",
    "    return chunks, embeddings\n",
    "\n",
    "\n",
    "def create_gpu_index(vecs, n_gpus, method):\n",
    "    \"\"\"\n",
    "    Create FAISS index on GPU(s).\n",
    "    To create a GPU index with FAISS, one first needs to create it on CPU then copy it on GPU. \n",
    "    Note that a \"flat\" index means that it is brute-force, with no approximation techniques.\n",
    "    \"\"\"\n",
    "    # Build flat CPU index.\n",
    "    if method=='l2':\n",
    "        cpu_index = faiss.IndexFlatL2(vecs.shape[1])  # Exact Search for L2\n",
    "    elif method=='ip':\n",
    "        cpu_index = faiss.IndexFlatIP(vecs.shape[1])  # Exact Search for Inner Product (also for cosine, just normalize vectors beforehand)\n",
    "    else:\n",
    "        print(\"Error: Please choose between L2 distance ('l2') or Inner Product ('ip') as brute-force method for exact search. Exiting...\")\n",
    "        sys.exit(0)\n",
    "    \n",
    "    # Convert to flat GPU index.\n",
    "    co = faiss.GpuMultipleClonerOptions()  # If using multiple GPUs, enable sharding so that the dataset is divided across the GPUs rather than replicated.\n",
    "    co.shard = True\n",
    "    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index,co=co, ngpu=n_gpus)  # Convert CPU index to GPU index.\n",
    "    \n",
    "    # Add vectors to GPU index.\n",
    "    gpu_index.add(vecs)\n",
    "\n",
    "    return gpu_index\n",
    "\n",
    "\n",
    "def load_questions(filepath, questions_type):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Load dataframe.\n",
    "    df = pd.read_csv(filepath, sep=\";\")\n",
    "\n",
    "    if questions_type is None:\n",
    "        # Get all questions and their associated answers.\n",
    "        questions = df.Question.values\n",
    "        answers = df.Answer.values\n",
    "    else:\n",
    "        # Get questions of given type and associated answers.\n",
    "        questions = df[df['QuestionType'] == questions_type].Question.values\n",
    "        answers = df[df['QuestionType'] == questions_type].Answer.values\n",
    "    \n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "def encode_sentences(model_name_or_path, cache, sentences):\n",
    "    \"\"\"\n",
    "    Given a list of sentences and a model, get the embeddings of theses sentences\n",
    "    as the average of the word embeddings of the last layer.\n",
    "    \"\"\"\n",
    "    print(\"   Loading pretrained model/tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "    model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True, cache_dir=cache) # Will output all hidden_states.\n",
    "\n",
    "    print(\"   Tokenizing sentences...\")\n",
    "    tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "    lengths = [len(i) for i in tokenized]\n",
    "    max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "    print(\"   Padding/Truncating sentences to {} tokens...\".format(max_len))\n",
    "    padded = pad_sequences(tokenized, maxlen=max_len, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    print(\"   Creating attention masks...\")\n",
    "    attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "    print(\"   Converting inputs to torch tensors...\")\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    print(\"   Encoding sentences...\")\n",
    "    with torch.no_grad():\n",
    "        # output is a 2-tuple where:\n",
    "        #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "        #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "        #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "    last_hidden_states = output[0]\n",
    "    sentence_embeddings = [torch.mean(embeddings[:torch.squeeze((masks == 1).nonzero(), dim=1).shape[0]], dim=0).numpy() for embeddings, masks in zip(last_hidden_states, attention_mask)]\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    \n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove common punctuations.\n",
    "    \"\"\"\n",
    "    return re.sub('([.,:;!?{}()])', r'', text)\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Remove punctuations.\n",
    "    processed = remove_punctuation(text)\n",
    "    \n",
    "    # Lower text.\n",
    "    processed = processed.lower()\n",
    "    \n",
    "    # Remove stopwords.\n",
    "    processed = [w for w in processed.split() if w not in stopwords.words('english')]\n",
    "    return processed\n",
    "\n",
    "\n",
    "def compute_score(chunk, answer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Get all words from answer present in the chunk.\n",
    "    words = [w for w in answer if w in chunk]\n",
    "    \n",
    "    # Define the score as the percentage of words from answer present in the chunk.\n",
    "    score = len(words)/len(answer)\n",
    "    return score\n",
    "\n",
    "\n",
    "def run(model_name_or_path, embeddings_filepath, questions_filepath, questions_type=None,\n",
    "        cache='/raid/antoloui/Master-thesis/Code/_cache/', method='l2', n_gpus=1, topk=10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"\\nLoad chunks and their embeddings...\") \n",
    "    chunks, embeddings = load_chunks(embeddings_filepath)\n",
    "    \n",
    "    print(\"\\nCreate FAISS (GPU) index...\")\n",
    "    index = create_gpu_index(vecs=embeddings, \n",
    "                             n_gpus=n_gpus, \n",
    "                             method=method)\n",
    "    \n",
    "    print(\"\\nLoad questions...\")\n",
    "    questions, answers = load_questions(questions_filepath, questions_type)\n",
    "    \n",
    "    print(\"\\nEncode questions...\")\n",
    "    quest_embeddings = encode_sentences(model_name_or_path=model_name_or_path, \n",
    "                                             cache=cache,\n",
    "                                             sentences=questions)\n",
    "    \n",
    "    print(\"\\nPerform evaluation...\")\n",
    "    # For each question-answer pair...\n",
    "    for i, (Q, E, A) in enumerate(zip(questions, quest_embeddings, answers)):\n",
    "        \n",
    "        print(\"\\nQUESTION {}: '{}'\".format(i, Q))\n",
    "        print(\"ANSWER: '{}'\".format(A))\n",
    "        \n",
    "        # Process the answer.\n",
    "        multiple_ans = A.split(';')\n",
    "        processed_ans = [process_text(ans) for ans in multiple_ans]\n",
    "\n",
    "        # Find topk chunks with FAISS search.\n",
    "        result_dist, result_idx = index.search(E.reshape(1,768), k=topk)\n",
    "\n",
    "        # For each result chunk, compute a score. \n",
    "        # The score si defined as the percentage of words in the answer that appears in that chunk. If multiple answers are possible, the max score is taken.\n",
    "        scores = []\n",
    "        for (idx,dist) in enumerate(zip(result_idx[0],result_dist[0])):\n",
    "\n",
    "            # Get the chunk and process it.\n",
    "            processed_chunk = process_text(chunks[idx])\n",
    "\n",
    "            # Compute a score for that chunk according to each answer.\n",
    "            scores_by_ans = [compute_score(processed_chunk, ans) for ans in processed_ans]\n",
    "\n",
    "            # Take the maximum score out of the possible answers.\n",
    "            score = max(scores_by_ans)\n",
    "\n",
    "            # Append it to the scores list for topk chunks.\n",
    "            scores.append(score)\n",
    "\n",
    "        # Get the final scores as (1) the mean score, (2) the max score.\n",
    "        max_score = max(scores)\n",
    "        mean_score = sum(scores)/len(scores)\n",
    "        print(\"  MAX SCORE: '{}'\".format(max_score))\n",
    "        print(\"  MEAN SCORE: '{}'\".format(mean_score))\n",
    "        \n",
    "    \n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\n",
    "    model_name_or_path='/raid/antoloui/Master-thesis/Code/_models/netbert-830000/',\n",
    "    embeddings_filepath='/raid/antoloui/Master-thesis/Data/QA/embeddings/netbert_embeddings.h5',\n",
    "    questions_filepath='/raid/antoloui/Master-thesis/Data/QA/questions.csv', \n",
    "    questions_type='Knowledge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(model_name_or_path='bert-base-cased',\n",
    "    embeddings_filepath='/raid/antoloui/Master-thesis/Data/QA/embeddings/bert_embeddings.h5',\n",
    "    questions_filepath='/raid/antoloui/Master-thesis/Data/QA/questions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
