{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data extraction\n",
    "\n",
    "Extract text contained in json files and save it in a dataframe for further pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoloui/anaconda3/envs/data/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "[nltk_data] Downloading package punkt to /home/antoloui/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy, en_core_web_sm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7203d1e24524fd3a71c17f4f0376234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 - Keys: {'uri', 'text'} - Documents: 11699\n",
      "File 2 - Keys: {'uri', 'text'} - Documents: 32072\n",
      "File 3 - Keys: {'uri', 'text'} - Documents: 8225\n",
      "File 4 - Keys: {'uri', 'text'} - Documents: 77258\n",
      "File 5 - Keys: {'uri', 'text'} - Documents: 46079\n",
      "File 6 - Keys: {'uri', 'text'} - Documents: 28106\n",
      "File 7 - Keys: {'uri', 'text'} - Documents: 27391\n",
      "File 8 - Keys: {'uri', 'text'} - Documents: 24143\n",
      "File 9 - Keys: {'uri', 'text'} - Documents: 22223\n",
      "File 10 - Keys: {'uri', 'text'} - Documents: 20979\n",
      "File 11 - Keys: {'uri', 'text'} - Documents: 57160\n",
      "File 12 - Keys: {'uri', 'text'} - Documents: 85900\n",
      "File 13 - Keys: {'uri', 'text'} - Documents: 793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the different keys in the json file\n",
    "for id_file in tqdm(range(1,14)):\n",
    "    file_path = \"../../Data/Original/\" + str(id_file) + \".json\"\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Loop over each document\n",
    "        keys = []\n",
    "        for i, doc in enumerate(data):\n",
    "            for key, value in doc.items():\n",
    "                keys.append(key)\n",
    "\n",
    "        myset = set(keys)\n",
    "        print(\"File {} - Keys: {} - Documents: {}\".format(id_file, myset, len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from 11699 documents in file '1.json'...\n",
      "Creating dataframe...\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "id_file = 1\n",
    "\n",
    "rows = []\n",
    "file_path = \"../../Data/Original/\" + str(id_file) + \".json\"\n",
    "\n",
    "with open(file_path) as f:\n",
    "    data = json.load(f)  # data is a list of dict of the form: {'text':['...'], 'uri':['...']}\n",
    "\n",
    "    print(\"Extracting text from {} documents in file '{}.json'...\".format(len(data), id_file))\n",
    "    for i, doc in enumerate(data):\n",
    "        text = doc.get('text') # Get the text of the current doc\n",
    "        if text is not None:\n",
    "            row_dict = {'Text': text[0], 'Length': len(text[0])}\n",
    "            rows.append(row_dict)\n",
    "\n",
    "print(\"Creating dataframe...\")\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of characters in a doc: 2621440\n",
      "Total number of docs: 11692\n"
     ]
    }
   ],
   "source": [
    "print(\"Max number of characters in a doc: {}\".format(df.Length.max()))\n",
    "print(\"Total number of docs: {}\".format(len(df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "In order to use \"create_pretraining_data.py\" from BERT repository, the input must be a plain text file, with one sentence per line and one blank line between documents:\n",
    "\n",
    "  * One sentence per line. These should ideally be actual sentences, not entire paragraphs or arbitrary spans of text. (Because we use the sentence boundaries for the \"next sentence prediction\" task).\n",
    "  * Blank lines between documents. Document boundaries are needed so that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  \n",
    "They advise to perform sentence segmentation with an off-the-shelf NLP toolkit such as spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Corpus Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning corpus of text...\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning corpus of text...\")\n",
    "df.Text = df.Text.replace('\\s+', ' ', regex=True)  # Remove duplicate spaces\n",
    "df.Text = df.Text.str.encode('ascii', 'ignore').str.decode('utf-8')   # Encode in ascii to remove weird characters such as \\uf0a7\n",
    "#df.Text = df.Text.str.lower()  # Lower case all strings\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_segmentation(doc_text):\n",
    "    \"\"\"\n",
    "    Given a string, segment it by sentences (performed by Spacy).\n",
    "    \"\"\"\n",
    "    nlp = en_core_web_sm.load()\n",
    "    nlp.max_length = 2621500  # because larger document has a size of 2621440 char\n",
    "    doc = nlp(doc_text)\n",
    "    sentences = list(doc.sents)\n",
    "    return [sent.text for sent in sentences]\n",
    "\n",
    "\n",
    "def nltk_segmentation(doc_text):\n",
    "    \"\"\"\n",
    "    Given a string, segment it by sentences (performed by nltk).\n",
    "    \"\"\"\n",
    "    return sent_tokenize(doc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Sentence cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_cleaning(list_sent):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Remove sequences of special characters\n",
    "    spec_char = set(',?;.:/=+%`¨*$€-_())°!§\\'\\\"&@#~®†ºπ‡¬≈©◊~∞µ…÷≠<>^')\n",
    "    list_sent = [' '.join([x for x in sent.split() if len(x)<=2 or not all(c in spec_char for c in x)]) for sent in list_sent]\n",
    "    \n",
    "    # If line begins with a number, remove the number   \n",
    "    list_sent = [sent.split(maxsplit=1)[1] if (len(sent.split(maxsplit=1))>1 and sent.split(maxsplit=1)[0].isdigit()) else sent for sent in list_sent]\n",
    "    \n",
    "    # If line begins with a unique special char, remove that char\n",
    "    list_sent = [sent.split(maxsplit=1)[1] if (len(sent.split(maxsplit=1))>1 and len(sent.split(maxsplit=1)[0])==1 and sent.split(maxsplit=1)[0] in spec_char) else sent for sent in list_sent]\n",
    "    \n",
    "    # Keep only sentences with more than 2 words and less than 200 words\n",
    "    list_sent = [sent for sent in list_sent if (len(sent.split())>2 and len(sent.split())<200)]\n",
    "    return list_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Plain text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_convert(list_sent):\n",
    "    \"\"\"\n",
    "    Given a list of string sentences, return one unique string where\n",
    "    sentences are separated by newlines.\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(list_sent)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Apply all cleaning functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3a9cb795d24c0daeeeab5d587f1a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27d1c6a365844dfab7615b151937620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenating all sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c31257cf6849b08a115e39264b0533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Segmenting sentences...\")\n",
    "df['Text'] = df['Text'].progress_apply(nltk_segmentation)\n",
    "print(\"Cleaning sentences...\")\n",
    "df['Text'] = df['Text'].progress_apply(sent_cleaning)\n",
    "print(\"Concatenating all sentences...\")\n",
    "df['Text'] = df['Text'].progress_apply(sent_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Concatenate all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sentences to output file (total of 102836 words)...\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\\n\\n\".join(df[\"Text\"])\n",
    "print(\"Saving sentences to output file (total of {} words)...\".format(len(sentences.split())))\n",
    "output_file = \"../../Data/Preprocessed/output.txt\"\n",
    "with open(output_file, \"w+\") as f:\n",
    "    f.write(sentences)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Check size of output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of 'all_text.txt': 16.16 GB\n",
      "Size of 'output.txt': 0.00 GB\n",
      "Size of 'text_1.txt': 1.84 GB\n",
      "Size of 'text_10.txt': 0.92 GB\n",
      "Size of 'text_11.txt': 2.14 GB\n",
      "Size of 'text_12.txt': 3.05 GB\n",
      "Size of 'text_13.txt': 0.00 GB\n",
      "Size of 'text_2.txt': 0.13 GB\n",
      "Size of 'text_3.txt': 0.36 GB\n",
      "Size of 'text_4.txt': 2.95 GB\n",
      "Size of 'text_5.txt': 1.13 GB\n",
      "Size of 'text_6.txt': 0.96 GB\n",
      "Size of 'text_7.txt': 0.87 GB\n",
      "Size of 'text_8.txt': 1.09 GB\n",
      "Size of 'text_9.txt': 0.73 GB\n"
     ]
    }
   ],
   "source": [
    "directory = '../../Data/Preprocessed/'\n",
    "files = os.listdir(directory).sort()\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = directory + filename\n",
    "        print(\"Size of '{}': {:.2f} GB\".format(filename, os.path.getsize(file_path)/(1024*1024*1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
