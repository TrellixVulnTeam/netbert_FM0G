{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Computer networking may be considered a branch of electrical engineering.',\n",
    "             'Computer networking is part of the electronics engineering field.',\n",
    "             'Computer networking is a lot of business.',\n",
    "             'Any data sent across a network requires time to travel from source to destination.',\n",
    "             'The information pushed to the network needs time to go from point A to point B.',\n",
    "             'The travel time of the data is instantaneous.',\n",
    "             'Firewalls are typically configured to reject access requests from unrecognized sources.',\n",
    "             'Firewalls are usually set up to refuse access requests from unknown sources.',\n",
    "             'Firewalls allow actions from all foreign sources.',\n",
    "             'Short sentence.',\n",
    "             'Very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "model_name_or_path = '/raid/antoloui/Master-thesis/Code/_models/netbert-1027000/'  #'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True, cache_dir ='../../cache') # Will output all hidden_states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "[CLS] Computer networking may be considered a branch of electrical engineering. [SEP]\n",
      "['[CLS]', 'Computer', 'networking', 'may', 'be', 'considered', 'a', 'branch', 'of', 'electrical', 'engineering', '.', '[SEP]']\n",
      "[101, 6701, 16074, 1336, 1129, 1737, 170, 3392, 1104, 6538, 3752, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization in multiple steps.\n",
    "marked_text = [\"[CLS] \" + sent + \" [SEP]\" for sent in sentences]\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in marked_text]\n",
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_text]\n",
    "\n",
    "# Tokenization in one step.\n",
    "tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "# Example.\n",
    "print(\"Example:\\n\")\n",
    "random.seed(2)\n",
    "print(random.choice(marked_text))\n",
    "random.seed(2)\n",
    "print(random.choice(tokenized_text))\n",
    "random.seed(2)\n",
    "print(random.choice(indexed_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding/Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding/Truncating sentences to 39 tokens...\n",
      "Example:\n",
      "    [  101  6701 16074  1336  1129  1737   170  3392  1104  6538  3752   119\n",
      "   102     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Define length of longest sentence in our dataset.\n",
    "lengths = [len(i) for i in tokenized]\n",
    "max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "# Pad each tokenized sentence according to the maximum length.\n",
    "print(\"Padding/Truncating sentences to {} tokens...\".format(max_len))\n",
    "padded = pad_sequences(tokenized, maxlen=max_len, dtype=\"long\", \n",
    "                      value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Example.\n",
    "random.seed(2)\n",
    "print(\"Example:\\n    {}\".format(random.choice(padded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "    [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "# Example.\n",
    "random.seed(2)\n",
    "print(\"Example:\\n    {}\".format(random.choice(attention_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of hidden_states: torch.Size([11, 39, 13, 768])\n",
      "   - Number of layers (+1 with initial token embeddings): 13\n",
      "   - Number of sentences: 11\n",
      "   - Number of tokens in a sentence: 39\n",
      "   - Dimension of an embedding : 768\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # output is a 2-tuple where:\n",
    "    #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "    #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "    #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get individual components of the output.\n",
    "last_hidden_states = output[0]\n",
    "pooler_output = output[1]\n",
    "hidden_states = output[2]\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Switch around the “layers” and “tokens” dimensions with permute.\n",
    "hidden_states = hidden_states.permute(1,2,0,3)\n",
    "\n",
    "# Print dimensions of output.\n",
    "print(\"Dimensions of hidden_states: {}\".format(hidden_states.size()))\n",
    "print(\"   - Number of layers (+1 with initial token embeddings): {}\".format(hidden_states.size(2)))\n",
    "print(\"   - Number of sentences: {}\".format(hidden_states.size(0)))\n",
    "print(\"   - Number of tokens in a sentence: {}\".format(hidden_states.size(1)))\n",
    "print(\"   - Dimension of an embedding : {}\".format(hidden_states.size(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embeddings (average last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 768)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "#\n",
    "# Note:\n",
    "#  - In practice, batches of sentences will be encoded at the same time. The problem is that the tokenization step will add as many [PAD] token as the longest sentence in the batch.\n",
    "#    If we simply compute the average of all tokens embeddings as the embedding of the sentence, the latter could be really messed up if it is a very small sentence in a batch of\n",
    "#    very long sentences, as the average embedding will converge towards the one of the [PAD] token (as it is the most represented in that sequence).\n",
    "#  - One solution would be to sort the sentences by length before encoding them. That way, the sentences in a batch sampled sequentially will approximately all have the same length.\n",
    "#    What I don't like with this solution is that we completely lose the inital order of the sentences, thing that we might want to preserve if we later store these embeddings to a\n",
    "#    dataframe with their associated sentence, and that we want to retrieve the neighboring sentences to a given one.\n",
    "#  - My solution is simply to perform the mean on the non-padded tokens. For that, I use the 'attention_mask' tensor to get the positions of the non-padded tokens (they all have an \n",
    "#    attention mask of 1), then get the lenght of that index array and perform the mean on the embeddings[:len].\n",
    "sentence_embeddings = [torch.mean(embeddings[:torch.squeeze((masks == 1).nonzero(), dim=1).shape[0]], dim=0).numpy() for embeddings, masks in zip(last_hidden_states, attention_mask)]\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings (sum up last four layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 21, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sentence, um the last four layers of each token as their embbeding.\n",
    "sentence_vecs = []\n",
    "for sent in hidden_states:\n",
    "    token_vecs = []\n",
    "    for token in sent:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs.append(np.array(sum_vec))\n",
    "    sentence_vecs.append(token_vecs)\n",
    "sentence_vecs = np.array(sentence_vecs)\n",
    "sentence_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
