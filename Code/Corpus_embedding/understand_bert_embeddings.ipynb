{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Computer networking may be considered a branch of electrical engineering.',\n",
    "             'Computer networking is part of the electronics engineering field.',\n",
    "             'Computer networking is a lot of business.',\n",
    "             'Any data sent across a network requires time to travel from source to destination.',\n",
    "             'The information pushed to the network needs time to go from point A to point B.',\n",
    "             'The travel time of the data is instantaneous.',\n",
    "             'Firewalls are typically configured to reject access requests from unrecognized sources.',\n",
    "             'Firewalls are usually set up to refuse access requests from unknown sources.',\n",
    "             'Firewalls allow actions from all foreign sources.',\n",
    "             'Short sentence.',\n",
    "             'Very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. With BertTokenizer (from transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Original sentence:  Computer networking may be considered a branch of electrical engineering.\n",
      "\n",
      "- Add special tokens ([CLS] and [SEP]):  [CLS] Computer networking may be considered a branch of electrical engineering. [SEP]\n",
      "\n",
      "- Convert words to WordPiece tokens:  ['[CLS]', 'Computer', 'networking', 'may', 'be', 'considered', 'a', 'branch', 'of', 'electrical', 'engineering', '.', '[SEP]']\n",
      "\n",
      "- Converting WordPiece tokens to their vocab ids (0 -> 30522):  [101, 6701, 16074, 1336, 1129, 1737, 170, 3392, 1104, 6538, 3752, 119, 102]\n",
      "\n",
      "- Padd/truncate tokenized sentences to 40 tokens:  [  101  6701 16074  1336  1129  1737   170  3392  1104  6538  3752   119\n",
      "   102     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "\n",
      "- Create attention masks:  [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "\n",
      "- Convert tokens and masks to pytorch tensors:\n",
      "  * Input:  tensor([  101,  6701, 16074,  1336,  1129,  1737,   170,  3392,  1104,  6538,\n",
      "         3752,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "  * Masks:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load BERT WrdPiece tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('/raid/antoloui/Master-thesis/Code/_models/netbert-1027000/')\n",
    "\n",
    "# Sentence example.\n",
    "sent_id = 0\n",
    "print(\"- Original sentence:  {}\\n\".format(sentences[sent_id]))\n",
    "\n",
    "marked_text = [\"[CLS] \" + sent + \" [SEP]\" for sent in sentences]\n",
    "print(\"- Add special tokens ([CLS] and [SEP]):  {}\\n\".format(marked_text[sent_id]))\n",
    "\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in marked_text]\n",
    "print(\"- Convert words to WordPiece tokens:  {}\\n\".format(tokenized_text[sent_id]))\n",
    "\n",
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_text]\n",
    "print(\"- Converting WordPiece tokens to their vocab ids (0 -> 30522):  {}\\n\".format(indexed_tokens[sent_id]))\n",
    "\n",
    "# Define length of longest sentence in our batch.\n",
    "lengths = [len(i) for i in indexed_tokens]\n",
    "max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "padded_tokens = pad_sequences(indexed_tokens, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "print(\"- Padd/truncate tokenized sentences to {} tokens:  {}\\n\".format(max_len, padded_tokens[sent_id]))\n",
    "\n",
    "attention_masks = np.where(padded_tokens != 0, 1, 0)  #returns ndarray which is 1 if padded_tokens != 0 is True and 0 if False.\n",
    "print(\"- Create attention masks:  {}\\n\".format(attention_masks[sent_id]))\n",
    "\n",
    "input_ids = torch.tensor(padded_tokens)  \n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "print(\"- Convert tokens and masks to pytorch tensors:\".format(input_ids[sent_id]))\n",
    "print(\"  * Input:  {}\".format(input_ids[sent_id]))\n",
    "print(\"  * Masks:  {}\".format(attention_masks[sent_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. With less steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Original sentence:  Computer networking may be considered a branch of electrical engineering.\n",
      "\n",
      "- Convert words to vocab ids (0 -> 30522):  [101, 6701, 16074, 1336, 1129, 1737, 170, 3392, 1104, 6538, 3752, 119, 102]\n",
      "\n",
      "- Padd/truncate tokenized sentences to 40 tokens:  [  101  6701 16074  1336  1129  1737   170  3392  1104  6538  3752   119\n",
      "   102     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "\n",
      "- Create attention masks:  [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "\n",
      "- Convert tokens and masks to pytorch tensors:\n",
      "  * Input:  tensor([  101,  6701, 16074,  1336,  1129,  1737,   170,  3392,  1104,  6538,\n",
      "         3752,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "  * Masks:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load BERT WrdPiece tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('/raid/antoloui/Master-thesis/Code/_models/netbert-1027000/')\n",
    "\n",
    "# Sentence example.\n",
    "sent_id = 0\n",
    "print(\"- Original sentence:  {}\\n\".format(sentences[sent_id]))\n",
    "\n",
    "# `encode` will:\n",
    "#   (1) Tokenize the sentence.\n",
    "#   (2) Prepend the `[CLS]` token to the start.\n",
    "#   (3) Append the `[SEP]` token to the end.\n",
    "#   (4) Map tokens to their IDs.\n",
    "indexed_tokens = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "print(\"- Convert words to vocab ids (0 -> 30522):  {}\\n\".format(indexed_tokens[sent_id]))\n",
    "\n",
    "# Define length of longest sentence in our batch.\n",
    "lengths = [len(i) for i in indexed_tokens]\n",
    "max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "padded_tokens = pad_sequences(indexed_tokens, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "print(\"- Padd/truncate tokenized sentences to {} tokens:  {}\\n\".format(max_len, padded_tokens[sent_id]))\n",
    "\n",
    "attention_masks = np.where(padded_tokens != 0, 1, 0)  #returns ndarray which is 1 if padded_tokens != 0 is True and 0 if False.\n",
    "print(\"- Create attention masks:  {}\\n\".format(attention_masks[sent_id]))\n",
    "\n",
    "input_ids = torch.tensor(padded_tokens)  \n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "print(\"- Convert tokens and masks to pytorch tensors:\".format(input_ids[sent_id]))\n",
    "print(\"  * Input:  {}\".format(input_ids[sent_id]))\n",
    "print(\"  * Masks:  {}\".format(attention_masks[sent_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. In one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Original sentence:  Computer networking may be considered a branch of electrical engineering.\n",
      "\n",
      "  * Input:  tensor([  101,  6701, 16074,  1336,  1129,  1737,   170,  3392,  1104,  6538,\n",
      "         3752,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "  * Masks:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT WrdPiece tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('/raid/antoloui/Master-thesis/Code/_models/netbert-1027000/')\n",
    "\n",
    "# Sentence example.\n",
    "sent_id = 0\n",
    "print(\"- Original sentence:  {}\\n\".format(sentences[sent_id]))\n",
    "\n",
    "# `encode_plus` will:\n",
    "#   (1) Tokenize the sentence.\n",
    "#   (2) Prepend the `[CLS]` token to the start.\n",
    "#   (3) Append the `[SEP]` token to the end.\n",
    "#   (4) Map tokens to their IDs.\n",
    "#   (5) Pad or truncate the sentence to `max_length`\n",
    "#   (6) Create attention masks for [PAD] tokens.\n",
    "results = [tokenizer.encode_plus(sent, add_special_tokens=True, max_length=512, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt') for sent in sentences]\n",
    "\n",
    "# Extract the token ids.\n",
    "input_ids = [sent_dict['input_ids'] for sent_dict in results]\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "\n",
    "# Extract the attention masks.\n",
    "attention_masks = [sent_dict['attention_mask'] for sent_dict in results]\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "print(\"  * Input:  {}\".format(input_ids[sent_id]))\n",
    "print(\"  * Masks:  {}\".format(attention_masks[sent_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. With BertWordPieceTokenizer (from tokenizers)\n",
    "NB: This one is much faster (they can encode 1GB of text in ~20sec on a standard server's CPU)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Original sentence:  Computer networking may be considered a branch of electrical engineering.\n",
      "\n",
      "- Convert words to vocab ids (0 -> 30522):  [101, 6701, 16074, 1336, 1129, 1737, 170, 3392, 1104, 6538, 3752, 119, 102]\n",
      "\n",
      "- Padd/truncate tokenized sentences to 40 tokens:  [  101  6701 16074  1336  1129  1737   170  3392  1104  6538  3752   119\n",
      "   102     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "\n",
      "- Create attention masks:  [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "\n",
      "- Convert tokens and masks to pytorch tensors:\n",
      "  * Input:  tensor([  101,  6701, 16074,  1336,  1129,  1737,   170,  3392,  1104,  6538,\n",
      "         3752,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "  * Masks:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Load tokenizer.\n",
    "tokenizer = BertWordPieceTokenizer('/raid/antoloui/Master-thesis/Code/_models/netbert-1027000/vocab.txt',\n",
    "                                    add_special_tokens=True,\n",
    "                                    lowercase=False,\n",
    "                                    clean_text=True, \n",
    "                                    handle_chinese_chars=True,\n",
    "                                    strip_accents=True)\n",
    "\n",
    "# Sentence example.\n",
    "sent_id = 0\n",
    "print(\"- Original sentence:  {}\\n\".format(sentences[sent_id]))\n",
    "\n",
    "# `encode` will:\n",
    "#   (1) Tokenize the sentence.\n",
    "#   (2) Prepend the `[CLS]` token to the start.\n",
    "#   (3) Append the `[SEP]` token to the end.\n",
    "#   (4) Map tokens to their IDs.\n",
    "outputs = [tokenizer.encode(sent) for sent in sentences]\n",
    "indexed_tokens = [out.ids for out in outputs]\n",
    "print(\"- Convert words to vocab ids (0 -> 30522):  {}\\n\".format(indexed_tokens[sent_id]))\n",
    "\n",
    "# Define length of longest sentence in our batch.\n",
    "lengths = [len(i) for i in indexed_tokens]\n",
    "max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "padded_tokens = pad_sequences(indexed_tokens, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "print(\"- Padd/truncate tokenized sentences to {} tokens:  {}\\n\".format(max_len, padded_tokens[sent_id]))\n",
    "\n",
    "attention_masks = np.where(padded_tokens != 0, 1, 0)  #returns ndarray which is 1 if padded_tokens != 0 is True and 0 if False.\n",
    "print(\"- Create attention masks:  {}\\n\".format(attention_masks[sent_id]))\n",
    "\n",
    "input_ids = torch.tensor(padded_tokens)  \n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "print(\"- Convert tokens and masks to pytorch tensors:\".format(input_ids[sent_id]))\n",
    "print(\"  * Input:  {}\".format(input_ids[sent_id]))\n",
    "print(\"  * Masks:  {}\".format(attention_masks[sent_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of hidden_states: torch.Size([11, 64, 13, 768])\n",
      "   - Number of layers (+1 with initial token embeddings): 13\n",
      "   - Number of sentences: 11\n",
      "   - Number of tokens in a sentence: 64\n",
      "   - Dimension of an embedding : 768\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# Load pretrained model.\n",
    "model = BertModel.from_pretrained(pretrained_model_name_or_path='/raid/antoloui/Master-thesis/Code/_models/netbert-1027000/', \n",
    "                                  output_hidden_states=True, # Will output all hidden_states.\n",
    "                                  cache_dir ='../../cache') \n",
    "\n",
    "with torch.no_grad():\n",
    "    # output is a 2-tuple where:\n",
    "    #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "    #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "    #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "    output = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "# Get individual components of the output.\n",
    "last_hidden_states = output[0]\n",
    "pooler_output = output[1]\n",
    "hidden_states = output[2]\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Switch around the “layers” and “tokens” dimensions with permute.\n",
    "hidden_states = hidden_states.permute(1,2,0,3)\n",
    "\n",
    "# Print dimensions of output.\n",
    "print(\"Dimensions of hidden_states: {}\".format(hidden_states.size()))\n",
    "print(\"   - Number of layers (+1 with initial token embeddings): {}\".format(hidden_states.size(2)))\n",
    "print(\"   - Number of sentences: {}\".format(hidden_states.size(0)))\n",
    "print(\"   - Number of tokens in a sentence: {}\".format(hidden_states.size(1)))\n",
    "print(\"   - Dimension of an embedding : {}\".format(hidden_states.size(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embeddings (average last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "#\n",
    "# Note:\n",
    "#  - In practice, batches of sentences will be encoded at the same time. The problem is that the tokenization step will add as many [PAD] token as the longest sentence in the batch.\n",
    "#    If we simply compute the average of all tokens embeddings as the embedding of the sentence, the latter could be really messed up if it is a very small sentence in a batch of\n",
    "#    very long sentences, as the average embedding will converge towards the one of the [PAD] token (as it is the most represented in that sequence).\n",
    "#  - One solution would be to sort the sentences by length before encoding them. That way, the sentences in a batch sampled sequentially will approximately all have the same length.\n",
    "#    What I don't like with this solution is that we completely lose the inital order of the sentences, thing that we might want to preserve if we later store these embeddings to a\n",
    "#    dataframe with their associated sentence, and that we want to retrieve the neighboring sentences to a given one.\n",
    "#  - My solution is simply to perform the mean on the non-padded tokens. For that, I use the 'attention_mask' tensor to get the positions of the non-padded tokens (they all have an \n",
    "#    attention mask of 1), then get the lenght of that index array and perform the mean on the embeddings[:len].\n",
    "sentence_embeddings = [torch.mean(embeddings[:torch.squeeze((masks == 1).nonzero(), dim=1).shape[0]], dim=0).numpy() for embeddings, masks in zip(last_hidden_states, attention_masks)]\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings (sum up last four layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 64, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sentence, sum the last four layers of each token as their embbeding.\n",
    "sentence_vecs = []\n",
    "for sent in hidden_states:\n",
    "    token_vecs = []\n",
    "    for token in sent:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs.append(np.array(sum_vec))\n",
    "    sentence_vecs.append(token_vecs)\n",
    "sentence_vecs = np.array(sentence_vecs)\n",
    "sentence_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
