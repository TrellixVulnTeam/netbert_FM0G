{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Computer networking may be considered a branch of electrical engineering.',\n",
    "             'Computer networking is part of the electronics engineering field.',\n",
    "             'Computer networking is a lot of business.',\n",
    "             'Any data sent across a network requires time to travel from source to destination.',\n",
    "             'The information pushed to the network needs time to go from point A to point B.',\n",
    "             'The travel time of the data is instantaneous.',\n",
    "             'Firewalls are typically configured to reject access requests from unrecognized sources.',\n",
    "             'Firewalls are usually set up to refuse access requests from unknown sources.',\n",
    "             'Firewalls allow actions from all foreign sources.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(model_name_or_path, sentences):\n",
    "    \"\"\"\n",
    "    Given a list of sentences an d a model, get the embeddings of theses sentences\n",
    "    as the average of the word embeddings of the last layer.\n",
    "    \"\"\"\n",
    "    print(\"   Setting up CUDA and GPUs...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    print(\"   Loading pretrained model/tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "    model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True, cache_dir ='../_cache') # Will output all hidden_states.\n",
    "\n",
    "    print(\"   Tokenizing sentences...\")\n",
    "    tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "    lengths = [len(i) for i in tokenized]\n",
    "    max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "    print(\"   Padding/Truncating sentences to {} tokens...\".format(max_len))\n",
    "    padded = pad_sequences(tokenized, maxlen=max_len, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    print(\"   Creating attention masks...\")\n",
    "    attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "    print(\"   Converting inputs to torch tensors...\")\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    print(\"   Encoding sentences...\")\n",
    "    with torch.no_grad():\n",
    "        # output is a 2-tuple where:\n",
    "        #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "        #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "        #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "    hidden_states = torch.stack(output[2], dim=0)\n",
    "\n",
    "    # Switch around the “layers” and “tokens” dimensions with permute.\n",
    "    hidden_states = hidden_states.permute(1,2,0,3)\n",
    "\n",
    "    # For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "    last_hidden_states = output[0]\n",
    "    sentence_embeddings = [torch.mean(embeddings, dim=0).numpy() for embeddings in last_hidden_states]\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "    # Create pandas dataframe.\n",
    "    cols = ['feat'+str(i) for i in range(sentence_embeddings.shape[1])]\n",
    "    df = pd.DataFrame(data=sentence_embeddings[:,:], columns=cols)\n",
    "    df['Sentence'] = sentences\n",
    "    return df\n",
    "\n",
    "print(\"BERT-base\")\n",
    "bert_df = encode_sentences('bert-base-cased', sentences)\n",
    "print(\"NetBERT\")\n",
    "netbert_df = encode_sentences('../_models/netbert/checkpoint-1027000/', sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
