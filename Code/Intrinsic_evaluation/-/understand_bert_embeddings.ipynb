{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Computer networking may be considered a branch of electrical engineering.',\n",
    "             'Computer networking is part of the electronics engineering field.',\n",
    "             'Computer networking is a lot of business.',\n",
    "             'Any data sent across a network requires time to travel from source to destination.',\n",
    "             'The information pushed to the network needs time to go from point A to point B.',\n",
    "             'The travel time of the data is instantaneous.',\n",
    "             'Firewalls are typically configured to reject access requests from unrecognized sources.',\n",
    "             'Firewalls are usually set up to refuse access requests from unknown sources.',\n",
    "             'Firewalls allow actions from all foreign sources.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "model_name_or_path = '../../_models/netbert/checkpoint-1027000/'  #'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True, cache_dir ='../../cache') # Will output all hidden_states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "[CLS] Computer networking is part of the electronics engineering field. [SEP]\n",
      "['[CLS]', 'Computer', 'networking', 'is', 'part', 'of', 'the', 'electronics', 'engineering', 'field', '.', '[SEP]']\n",
      "[101, 6701, 16074, 1110, 1226, 1104, 1103, 11216, 3752, 1768, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization in multiple steps.\n",
    "marked_text = [\"[CLS] \" + sent + \" [SEP]\" for sent in sentences]\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in marked_text]\n",
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_text]\n",
    "\n",
    "# Tokenization in one step.\n",
    "tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "# Example.\n",
    "print(\"Example:\\n\")\n",
    "random.seed(42)\n",
    "print(random.choice(marked_text))\n",
    "random.seed(42)\n",
    "print(random.choice(tokenized_text))\n",
    "random.seed(42)\n",
    "print(random.choice(indexed_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding/Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding/Truncating sentences to 21 tokens...\n",
      "Example:\n",
      "    [  101  6701 16074  1110  1226  1104  1103 11216  3752  1768   119   102\n",
      "     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Define length of longest sentence in our dataset.\n",
    "lengths = [len(i) for i in tokenized]\n",
    "max_len = max(lengths) if max(lengths) <= 512 else 512\n",
    "\n",
    "# Pad each tokenized sentence according to the maximum length.\n",
    "print(\"Padding/Truncating sentences to {} tokens...\".format(max_len))\n",
    "padded = pad_sequences(tokenized, maxlen=max_len, dtype=\"long\", \n",
    "                      value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(padded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "    [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "# Example.\n",
    "random.seed(42)\n",
    "print(\"Example:\\n    {}\".format(random.choice(attention_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of hidden_states: torch.Size([9, 21, 13, 768])\n",
      "   - Number of layers (+1 with initial token embeddings): 9\n",
      "   - Number of sentences: 21\n",
      "   - Number of tokens in a sentence: 13\n",
      "   - Dimension of an embedding : 768\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # output is a 2-tuple where:\n",
    "    #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "    #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "    #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get individual components of the output.\n",
    "last_hidden_states = output[0]\n",
    "pooler_output = output[1]\n",
    "hidden_states = output[2]\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Switch around the “layers” and “tokens” dimensions with permute.\n",
    "hidden_states = hidden_states.permute(1,2,0,3)\n",
    "\n",
    "# Print dimensions of output.\n",
    "print(\"Dimensions of hidden_states: {}\".format(hidden_states.size()))\n",
    "print(\"   - Number of layers (+1 with initial token embeddings): {}\".format(hidden_states.size(0)))\n",
    "print(\"   - Number of sentences: {}\".format(hidden_states.size(1)))\n",
    "print(\"   - Number of tokens in a sentence: {}\".format(hidden_states.size(2)))\n",
    "print(\"   - Dimension of an embedding : {}\".format(hidden_states.size(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embeddings (average last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "sentence_embeddings = [torch.mean(embeddings, dim=0).numpy() for embeddings in last_hidden_states]\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings (sum up last four layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 21, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each sentence, um the last four layers of each token as their embbeding.\n",
    "sentence_vecs = []\n",
    "for sent in hidden_states:\n",
    "    token_vecs = []\n",
    "    for token in sent:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs.append(np.array(sum_vec))\n",
    "    sentence_vecs.append(token_vecs)\n",
    "sentence_vecs = np.array(sentence_vecs)\n",
    "sentence_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
