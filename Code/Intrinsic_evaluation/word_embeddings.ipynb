{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(model_name_or_path, sentences):\n",
    "    \"\"\"\n",
    "    Given a list of sentences an d a model, get the embeddings of theses sentences\n",
    "    as the average of the word embeddings of the last layer.\n",
    "    \"\"\"\n",
    "    print(\"   Loading pretrained model/tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "    model = BertModel.from_pretrained(model_name_or_path, output_hidden_states=True, cache_dir ='../_cache') # Will output all hidden_states.\n",
    "\n",
    "    print(\"   Tokenizing sentences...\")\n",
    "    tokenized = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "\n",
    "    max_len = 0\n",
    "    for i in tokenized:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    print(\"   Maximum length in dataset: {}\".format(max_len))\n",
    "\n",
    "    print(\"   Padding/Truncating sentences according to the maximum length...\")\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized])\n",
    "\n",
    "    print(\"   Creating attention masks...\")\n",
    "    attention_mask = np.where(padded != 0, 1, 0)  #returns ndarray which is 1 if padded != 0 is True and 0 if False.\n",
    "\n",
    "    print(\"   Converting inputs to torch tensors...\")\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    print(\"   Encoding sentences...\")\n",
    "    with torch.no_grad():\n",
    "        # output is a 2-tuple where:\n",
    "        #  - output[0] is the last_hidden_state, i.e a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "        #  - output[1] is the pooler_output, i.e. a tensor of shape (batch_size, hidden_size) being the last layer hidden-state of the first token of the sequence (classification token).\n",
    "        #  - output[2] are all hidden_states, i.e. a 13-tuple of torch tensors of shape (batch_size, sequence_length, hidden_size): 12 encoders-outputs + initial embedding outputs.\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "    hidden_states = torch.stack(output[2], dim=0)\n",
    "\n",
    "    # Switch around the “layers” and “tokens” dimensions with permute.\n",
    "    hidden_states = hidden_states.permute(1,2,0,3)\n",
    "\n",
    "    # For each sentence, take the embeddings of its word from the last layer and represent that sentence by their average.\n",
    "    last_hidden_states = output[0]\n",
    "    sentence_embeddings = [torch.mean(embeddings, dim=0).numpy() for embeddings in last_hidden_states]\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "    # Create pandas dataframe.\n",
    "    cols = ['feat'+str(i) for i in range(sentence_embeddings.shape[1])]\n",
    "    df = pd.DataFrame(data=sentence_embeddings[:,:], columns=cols)\n",
    "    df['Sentence'] = sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization in multiple steps.\n",
    "marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Tokenization in one step.\n",
    "tokenized = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "tokenized = torch.tensor([tokenized])\n",
    "\n",
    "# Encode it.\n",
    "with torch.no_grad():\n",
    "    output = model(tokenized)\n",
    "    \n",
    "# Get all hidden states.\n",
    "hidden_states = output[2]\n",
    "hidden_states = torch.stack(hidden_states, dim=0)\n",
    "hidden_states = torch.squeeze(hidden_states, dim=1)\n",
    "hidden_states = hidden_states.permute(1,0,2)\n",
    "print(\" - 'hidden_states' is a {} tensor.\".format(hidden_states.size()))\n",
    "\n",
    "# Sum the last four layers.\n",
    "token_vecs_sum = []\n",
    "for token in hidden_states:\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "print(\" - 'token_vecs_sum' is a ({}, {}) array.\".format(len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\n",
    "# Get idx of words of interest 'bank'.\n",
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)\n",
    "\n",
    "# Calculate the cosine similarity between the word bank in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Mark\n",
    "DNS\n",
    "mDNS\n",
    "\n",
    "LAN\n",
    "WAN\n",
    "CAN\n",
    "MAN\n",
    "\n",
    "ACL\n",
    "PACL\n",
    "VACL\n",
    "\n",
    "HSRP\n",
    "VRRP\n",
    "GLBP\n",
    "\n",
    "EIGRP\n",
    "IGRP\n",
    "BGP\n",
    "EBGP\n",
    "\n",
    "STP\n",
    "DTP\n",
    "VTP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
